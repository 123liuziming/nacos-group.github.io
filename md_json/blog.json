{
  "zh-cn": [
    {
      "filename": "DNS-SD.md",
      "__html": "<p>当前，微服务架构已经成为企业尤其是互联网企业技术选型的一个重要参考。微服务架构中涉及到很多模块，本文将重点介绍微服务架构的服务注册与发现以及如何基于DNS做服务发现。最后，简单介绍下阿里巴巴内部是如何基于DNS做服务发现的。</p>\n<h2>服务注册与发现</h2>\n<p>微服务体系中，服务注册与服务发现是两个最核心的模块。服务A调用服务B时，需要通过服务发现模块找到服务B的IP和端口列表，而服务B的实例在启动时需要把提供服务的IP和端口注册到服务注册中心。一个典型的结构如下图：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185623334-8fbf69bb-00df-4202-a951-c1ee9caace88.png\" alt=\"image.png | left | 733x470\"></p>\n<h3>服务注册</h3>\n<p>目前，流行的注册中心比较多，常见的有zookeeper、ectd、consul、eureka等。服务注册通常有三种：自注册、第三方注册、注册中心主动同步。</p>\n<ul>\n<li>自注册\n自注册，顾名思义，就是服务提供方在启动服务时自己把提供服务的IP和端口发送到注册中心，并通过心跳方式维持健康状态；服务下线时，自己把相应的数据删除。典型的像使用eureka客户端发布微服务。</li>\n<li>第三方注册\n第三方注册是指，存在一个第三方的系统负责在服务启动或停止时向注册中心增加或删除服务数据。典型的用法是devops系统或容器调度系统主动调注册中心接口注册服务；</li>\n<li>注册中心主动同步\n与第三方注册方式类似，主动注册方式是指注册中心和调度或发布系统打通，主动同步最新的服务IP列表；一个例子是kubernetes体系中，coredns订阅api server数据。</li>\n</ul>\n<h3>服务发现</h3>\n<p>在真正发起服务调用前，调用方需要从注册中心拿到相应服务可用的IP和端口列表，即服务发现。服务发现从对应用的侵入性上可以分为两大类：</p>\n<ul>\n<li>SDK-Based\n这类的服务发现方式，需要调用方依赖相应的SDK，显式调用SDK代码才可以实现服务调用，即对业务有侵入性，典型例子如eureka、zookeeper等。</li>\n<li>DNS-Based\nDNS本身是一种域名解析系统，可以满足简单的服务发现场景，如双方约定好端口、序列化协议等等。但是，这远远不能满足真正的微服务场景需求。近几年，基于dns的服务发现渐渐被业界提了出来。后面将重点介绍基于DNS的服务发现及阿里巴巴的实践。</li>\n</ul>\n<h2>基于DNS的服务发现</h2>\n<p>DNS协议是目前最为通用的协议之一，几乎所有操作系统都会有DNS客户端实现。所以，其天然具有跨语言特性。这也是快速接入微服务体系最快的一个方式。要基于DNS做服务发现，首先注册中心的数据应该可以以DNS的数据格式暴露出来。让任何系统的DNS 客户端都可以通过DNS协议获取服务列表。</p>\n<p>基于DNS的服务发现方式，大致可以归结两类：</p>\n<h3>独立DNS服务器</h3>\n<p>独立DNS Server模式的基本架构如下图：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185653048-094f815d-47af-409e-ac21-6662e20e76f4.png\" alt=\"image.png | left | 747x469\"></p>\n<p>如上图所示，这种架构中，需要独立的DNS服务器。DNS服务器从注册中心获取所有已注册的服务及对应的IP端口列表。调用方Service A 通过DNS查询某个服务下的IP列表，然后发起调用。</p>\n<p>这种类型的服务发现方式优缺点分别如下：</p>\n<ul>\n<li>优点\n<ul>\n<li>集中的DNS服务器，便于升级维护</li>\n</ul>\n</li>\n<li>缺点\n<ul>\n<li>对DNS服务器性能要求高</li>\n<li>这种情况下一般需要LVS设备为DNS服务器集群做请求转发，存在单点问题</li>\n</ul>\n</li>\n</ul>\n<h3>DNS Filter</h3>\n<p>DNS Filter模式我们定义为把DNS服务器集成到服务调用方机器或容器里，如下图所示：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185676597-acaebac3-2b65-48a3-bdbf-7d27af7f1038.png\" alt=\"image.png | left | 747x469\"></p>\n<p>这种模式中，首先要保证ServiceA的DNS查询都被拦截到本机的DNS服务器上（127.0.0.1:53），在获取到服务的IP列表后发起调用。由于这种方式把DNS服务器前置到实际调用的机器上，所以它解决了独立DNS服务器模式的单点问题，完全P2P的模式。但由于需要在应用机器上安装DNS服务器，其维护和升级成本较前者高一些。</p>\n<h3>阿里基于DNS的服务发现实践</h3>\n<p>阿里巴巴早在2014年就开始了基于DNS做服务发现的尝试了，现在已经形成了较为成熟的模式。阿里内部以VIPServer作为注册中心，并开发了DNS Filter，部署在应用容器内。目前已经有超过20w个机器或容器上安装了DNS Filter，支持了几乎所有REST服务发现。</p>\n<h4>注册中心 VIPServer</h4>\n<p>VIPServer是阿里中间件软负载团队自研的服务注册中心，按照第一章的分类，VIPServer同时支持三种模式的服务注册，并且均有相当规模的应用。除此之外，VIPServer具备如下特性：</p>\n<ul>\n<li>主动与被动健康检查\nVIPServer同时支持主动与被动健康检查。主动健康检查是指VIPserver服务端主动定期发送健康检查探测包，探测服务提供方是否可以正常提供服务。用户可以配置多种健康检查方式，自定义探测端口和探测URL（HTTP）。主动探测的好处在于服务提供方不用做任何改动即可快融入微服务架构。被动健康检查则是指服务提供方主动注册自己的IP、端口和服务名等信息，并通过心跳方式保持活性。</li>\n<li>多种负载均衡策略\n同时，VIPServer支持多种负载均衡策略，包括权重、同机房、同地域、同环境等等，是阿里异地多活项目的核心系统之一。后面会有专门的文章介绍如何基于VIPServer实现异地多活，这里不再赘述。</li>\n<li>多重容灾保护策略\nVIPServer提供了多种容灾保护策略，可以有效减少人为或者底层系统（网络等）异常带来的影响。这些容灾保护包括：\n<ul>\n<li>客户端缓存，即使VIPServer服务端挂掉也不影响应用的正常调用；</li>\n<li>服务端保护阈值，有效防止应用因压力过大而发生雪崩；</li>\n<li>客户端容灾目录，提供人工干预服务IP列表的能力；</li>\n<li>客户端空列表保护，有效防止人为误删IP列表操作或VIPServer服务端异常</li>\n</ul>\n</li>\n</ul>\n<h4>DNS-F客户端</h4>\n<p>出于性能的考虑，我们采用了第二种DNS Filter的服务发现模式。为此，我们单独开发了DNS-F客户端，DNS-F客户端跟应用部署在同一个主机或容器内。其工作原理如下图所示：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185700084-16f2c073-0c2b-49c0-b85b-ce2ca1163326.png\" alt=\"image.png | left | 716x392\"></p>\n<ul>\n<li>首先，应用ServiceA通过DNS查询获取到ServiceB的可用IP列表</li>\n<li>DNS-F会拦截到ServiceA的查询请求，判断自己是否该查询的答案，如果有（服务已在VIPServer中注册）则直接返回IP列表；</li>\n<li>如果查询的服务在VIPServer中没有注册，DNS-F把DNS查询转发给系统的nameserver，由真正的DNS系统解析；</li>\n</ul>\n<h2>总结</h2>\n<p>本文介绍了微服务架构中如何基于DNS做服务发现。首先，介绍了服务法注册与发现的概念、服务注册与发现的几种方式及其优缺点；然后，介绍基于DNS的服务发现的两种方式及其优缺点；最后，介绍了阿里巴巴基于DNS做服务发现的实践，主要是基于自研的软负载系统VIPServer。基于DNS的服务发现要解决的问题远不止本文描述的这些，敬请期待后续系列文章（：。</p>\n"
    },
    {
      "filename": "Eureka2.md",
      "__html": "<p>近日，Netflix 公司在 github 上公告 Eureka 2.0 开源工作停止，继续使用风险自负。这一消息在spring cloud 开发者中引起了一些担心和忧虑。</p>\n<p>同时在阿里巴巴正式宣布其开源计划， 将通过 Nacos 项目将阿里巴巴在建设共享服务体系中使用的服务发现、配置及服务管理平台贡献给开源社区，通过打造 Dubbo + Nacos 的经典组合进一步释放 Dubbo 在云原生及 Service Mesh 时代中，在大规模微服务治理、流量治理、服务集成与服务共享等服务平台能力建设上的威力，同时 Nacos 会非常关注对主流开源社区，如 Spring Cloud 和 Kubernetes 云原生体系的无缝对接与支持。</p>\n<p>ANS (alibaba naming service) 是 nacos 组件中的服务发现部分。开源流程正在紧张地进行中。同时 ANS 与 spring cloud 结合的 spring cloud starter ans 也将同时开源到 spring cloud <a href=\"https://github.com/spring-cloud-incubator/spring-cloud-alibabacloud\">官方孵化器</a>。</p>\n<p>ANS 同样也支持 spring cloud 应用的服务注册与发现，同时默认集成了负载均衡组件 Ribbon，Eureka 用户可以简单地通过替换 maven 项目中 pom.xml 文件中的依赖来实现无缝迁移。</p>\n<p>想抢先体验？来试试商业版的 EDAS Ans starter 吧！</p>\n<h2>快速开始</h2>\n<h3>服务提供者</h3>\n<ol>\n<li>\n<p>创建一个 Spring Cloud 工程，命名为 service-provider。这里我们以 spring boot 1.5.8 和 spring cloud Dalston.SR4 为例。在 pom.xml 中引入需要的依赖内容。<br>\n其他版本如 spring boot 2 + spring cloud Finchley 也同样支持，请您自行修改版本号和替换相应的组件依赖。</p>\n<pre><code> &lt;parent&gt;\n \t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n \t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\n \t&lt;version&gt;1.5.8.RELEASE&lt;/version&gt;\n \t&lt;relativePath/&gt;\n &lt;/parent&gt;\n\n\n &lt;dependencies&gt;\n \t&lt;dependency&gt;\n \t\t&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n \t\t&lt;artifactId&gt;spring-cloud-starter-ans&lt;/artifactId&gt;\n \t\t&lt;version&gt;1.1.3&lt;/version&gt;\n \t&lt;/dependency&gt;\n \t&lt;dependency&gt;\n \t\t&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n \t\t&lt;artifactId&gt;spring-cloud-alibaba-edas-starter&lt;/artifactId&gt;\n \t\t&lt;version&gt;1.1.3&lt;/version&gt;\n \t&lt;/dependency&gt;\n &lt;/dependencies&gt;\n\n &lt;dependencyManagement&gt;\n \t&lt;dependencies&gt;\n \t\t&lt;dependency&gt;\n \t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n \t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n \t\t\t&lt;version&gt;Dalston.SR4&lt;/version&gt;\n \t\t\t&lt;type&gt;pom&lt;/type&gt;\n \t\t\t&lt;scope&gt;import&lt;/scope&gt;\n \t\t&lt;/dependency&gt;\n \t&lt;/dependencies&gt;\n &lt;/dependencyManagement&gt;\n</code></pre>\n</li>\n<li>\n<p>编码服务提供端的启动类，其中 @EnableDiscoveryClient 注解表明此应用需开启服务注册与发现功能。</p>\n<pre><code> @SpringBootApplication\n @EnableDiscoveryClient\n public class ServerApplication {\n \n     public static void main(String[] args) {\n         SpringApplication.run(ServerApplication.class, args);\n     }\n }\n</code></pre>\n</li>\n<li>\n<p>既然是服务提供者，所以我们还需要提供一个简单的服务。这里 EchoController 的逻辑很简单，将收到的参数回显给调用者。</p>\n<pre><code> @RestController\n public class EchoController {\n     @RequestMapping(value = &quot;/echo/{string}&quot;, method = RequestMethod.GET)\n     public String echo(@PathVariable String string) {\n         return string;\n     }\n }\n</code></pre>\n</li>\n<li>\n<p>权限配置，配置阿里云账号的 AccessKey、SecretKey，以及 EDAS 的命名空间信息。</p>\n<ol>\n<li>\n<p>配置阿里云 AccessKey 和 SecretKey</p>\n<p>登陆 <a href=\"https://usercenter.console.aliyun.com/#/accesskey\">阿里云AK管理控制台</a>。找到 <code>用户信息管理</code> 下的 <code>安全信息管理</code>。</p>\n<p>复制 <code>AccessKey ID</code> 和 <code>Access Key Secret</code>，分别对应配置项中的 alibaba.cloud.access-key 和 alibaba.cloud.secret-key。</p>\n<p>安全凭证信息格式如下：</p>\n<pre><code> alibaba.cloud.access-key=xxxxxxxxxx\n alibaba.cloud.secret-key=xxxxxxxxxx\n</code></pre>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/54319/1531104058520-831f4dba-6279-4bd3-9632-899d29830868.png\" alt=\"2.png\"></p>\n</li>\n<li>\n<p>配置 EDAS 的命名空间</p>\n<p>登录 <a href=\"https://edas.console.aliyun.com/#/home\">EDAS 控制台</a>。未开通EDAS的用户需要先开通EDAS，<strong>EDAS 标准版促销进行中，每月仅需1元。</strong></p>\n<p>在左侧导航栏中单击<strong>命名空间</strong>。在命名空间列表页面选择<strong>地域</strong>，并找到您想发布到的命名空间，复制其<strong>命名空间ID</strong>，对应配置项中的 <strong>alibaba.edas.namespace</strong>。</p>\n<pre><code> alibaba.edas.namespace=xxxxxxxxxx\n</code></pre>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/54319/1531104049757-c3d2aefd-5d57-4434-a04f-2cd939cf58b0.png\" alt=\"1.png\"></p>\n</li>\n</ol>\n</li>\n<li>\n<p>综上，我们的配置文件 application.properties 内容最后是这样的。</p>\n<pre><code> spring.application.name=service-provider\n server.port=18081\n alibaba.cloud.access-key=xxxxxxxxxx\n alibaba.cloud.secret-key=xxxxxxxxxx\n alibaba.edas.namespace=cn-hangzhou\n</code></pre>\n</li>\n<li>\n<p>启动 service-provider 服务，在 EDAS 页面查看服务注册的信息。</p>\n<p>可以看到服务列表中已经存在的实例 service-provider，从详情页可以看到IP地址是本机的地址192.168.0.107,端口是 18081。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/jpeg/54319/1531104455581-1d9521aa-927e-4292-9d83-b1057ab67c02.jpeg\" alt=\"3.JPG\"></p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/54319/1531104077485-48d3d67b-584d-4729-aef2-2fbb64c0b846.png\" alt=\"4.png\"></p>\n</li>\n</ol>\n<h3>服务消费者</h3>\n<p>这个例子中，我们将不仅仅是演示服务发现的功能，同时还将演示 ANS 服务发现 与 RestTemplate、AsyncRestTemplate 和 FeignClient 这三个客户端是如何结合的。因为实际使用中，我们更多使用的是用这三个客户端进行服务调用。</p>\n<ol>\n<li>\n<p>创建一个 Spring Cloud 工程，命名为 service-consumer。首先在 pom.xml 中引入需要的依赖内容：</p>\n<pre><code> &lt;parent&gt;\n \t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n \t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\n \t&lt;version&gt;1.5.8.RELEASE&lt;/version&gt;\n \t&lt;relativePath/&gt;\n &lt;/parent&gt;\n\n &lt;dependencies&gt;\n \t&lt;dependency&gt;\n \t\t&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n \t\t&lt;artifactId&gt;spring-cloud-starter-ans&lt;/artifactId&gt;\n \t\t&lt;version&gt;1.1.3&lt;/version&gt;\n \t&lt;/dependency&gt;\n \t&lt;dependency&gt;\n \t\t&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n \t\t&lt;artifactId&gt;spring-cloud-alibaba-edas-starter&lt;/artifactId&gt;\n \t\t&lt;version&gt;1.1.3&lt;/version&gt;\n \t&lt;/dependency&gt;\n &lt;/dependencies&gt;\n\n &lt;dependencyManagement&gt;\n \t&lt;dependencies&gt;\n \t\t&lt;dependency&gt;\n \t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n \t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n \t\t\t&lt;version&gt;Dalston.SR4&lt;/version&gt;\n \t\t\t&lt;type&gt;pom&lt;/type&gt;\n \t\t\t&lt;scope&gt;import&lt;/scope&gt;\n \t\t&lt;/dependency&gt;\n \t&lt;/dependencies&gt;\n &lt;/dependencyManagement&gt;\n</code></pre>\n<p>因为在这里我们要演示 FeignClient 的使用，所以与 service-provider 相比，pom.xml 文件中的依赖增加了一个 spring-cloud-starter-feign。</p>\n</li>\n<li>\n<p>完成以下四个步骤，配置好 RestTemplate 、 AsyncRestTemplate 和 FeignClient。</p>\n<ol>\n<li>\n<p>在使用 FeignClient 之前，我们还需要对它的配置，配置服务名以及方法对应的 HTTP 请求，示例代码如下</p>\n<pre><code> @FeignClient(name = &quot;service-provider&quot;)\n public interface EchoService {\n \t@RequestMapping(value = &quot;/echo/{str}&quot;, method = RequestMethod.GET)\n \tString echo(@PathVariable(&quot;str&quot;) String str);\n }\n</code></pre>\n</li>\n<li>\n<p>在启动类中，使用 @EnableDiscoveryClient 注解启用服务注册与发现</p>\n</li>\n<li>\n<p>在启动类中，使用 @EnableFeignClients 注解激活 FeignClient</p>\n</li>\n<li>\n<p>在启动类中，添加 @LoadBalanced 注解将 RestTemplate 与 AsyncRestTemplate 与服务发现结合。</p>\n</li>\n</ol>\n<p>最终启动类的代码如下</p>\n<pre><code> @SpringBootApplication\n @EnableDiscoveryClient\n @EnableFeignClients\n public class ConsumerApplication {\n \t@LoadBalanced\n \t@Bean\n \tpublic RestTemplate restTemplate() {\n \t\treturn new RestTemplate();\n \t}\n \n \t@LoadBalanced\n \t@Bean\n \tpublic AsyncRestTemplate asyncRestTemplate(){\n \t\treturn new AsyncRestTemplate();\n \t}\n \t\n \tpublic static void main(String[] args) {\n \t\tSpringApplication.run(ConsumerApplication.class, args);\n \t}\n \n }\n</code></pre>\n</li>\n<li>\n<p>创建一个Controller，供我们演示和验证服务发现功能使用。</p>\n<pre><code> @RestController\n public class TestController {\n \n     @Autowired\n     private RestTemplate restTemplate;\n     @Autowired\n     private AsyncRestTemplate asyncRestTemplate;\n     @Autowired\n     private  EchoService echoService;\n \n     @RequestMapping(value = &quot;/echo-rest/{str}&quot;, method = RequestMethod.GET)\n     public String rest(@PathVariable String str) {\n         return restTemplate.getForObject(&quot;http://service-provider/echo/&quot; + str, String.class);\n     }\n     @RequestMapping(value = &quot;/echo-async-rest/{str}&quot;, method = RequestMethod.GET)\n     public String asyncRest(@PathVariable String str) throws Exception{\n         ListenableFuture&lt;ResponseEntity&lt;String&gt;&gt; future = asyncRestTemplate.\n                 getForEntity(&quot;http://service-provider/echo/&quot;+str, String.class);\n         return future.get().getBody();\n     }\n     @RequestMapping(value = &quot;/echo-feign/{str}&quot;, method = RequestMethod.GET)\n     public String feign(@PathVariable String str) {\n         return echoService.echo(str);\n     }\n \n }\n</code></pre>\n</li>\n<li>\n<p>最后，添加应用基本配置和阿里云 AK、SK 以及 EDAS 的 namespace。</p>\n<pre><code> spring.application.name=service-consumer\n server.port=18082\n alibaba.cloud.access-key=xxxxxxxxxx\n alibaba.cloud.secret-key=xxxxxxxxxx\n alibaba.edas.namespace=cn-hangzhou\n</code></pre>\n</li>\n<li>\n<p>启动服务，首先查看EDAS控制台，查询服务，可以看到，服务注册成功了。</p>\n<p>再对我们的演示 API 分别进行调用，可以看到调用都成功了。\n<img src=\"https://cdn.yuque.com/lark/0/2018/png/54319/1531104088354-7cc0fae7-95cf-4c62-a9ea-1b9a53908949.png\" alt=\"5.png\"></p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/54319/1531104133698-f9621e80-7cb1-4789-a88a-929f720b5c2b.png\" alt=\"6.png\"></p>\n</li>\n</ol>\n<h2>Demo 下载</h2>\n<p><a href=\"http://edas-public.oss-cn-hangzhou.aliyuncs.com/install_package/demo/ans-service-provider.zip\">service-provider</a></p>\n<p><a href=\"http://edas-public.oss-cn-hangzhou.aliyuncs.com/install_package/demo/ans-service-consumer.zip\">service-consumer</a></p>\n<h2>更多配置项</h2>\n<table>\n<thead>\n<tr>\n<th>配置项</th>\n<th>key</th>\n<th>默认值</th>\n<th>说明</th>\n<th>补充说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>服务名</td>\n<td>spring.cloud.ans.doms</td>\n<td><a href=\"http://spring.application.name\">spring.application.name</a></td>\n<td>当此项未配置时，默认从spring.application.name中获取</br>需要发布多个服务时，中间用英文的 <code>,</code> 号隔开</td>\n<td>production</td>\n</tr>\n<tr>\n<td>是否注册</td>\n<td>spring.cloud.ans.register-enabled</td>\n<td>true</td>\n<td>当只需要发现，不需要注册时，可以通过将值设置成false来关闭注册</td>\n<td>production</td>\n</tr>\n<tr>\n<td>想要注册的IP</td>\n<td>spring.cloud.ans.ip</td>\n<td>无</td>\n<td>当需要指定本机注册的IP时，通过此值来配置，优先级最高</td>\n<td>production</td>\n</tr>\n<tr>\n<td>想要注册的IP所属的网卡</td>\n<td>spring.cloud.ans.interface-name</td>\n<td>无</td>\n<td>当确定需要发布哪块网卡对应的IP地址时，通过此参数配置，值为网卡名</td>\n<td>production</td>\n</tr>\n<tr>\n<td>想要注册的端口</td>\n<td>spring.cloud.ans.port</td>\n<td>无</td>\n<td>想要注册的端口</td>\n<td>production</td>\n</tr>\n<tr>\n<td>注册的权重</td>\n<td>spring.cloud.ans.weight</td>\n<td>1</td>\n<td>数值越大权重越高，取值范围为</td>\n<td>test</td>\n</tr>\n<tr>\n<td>集群</td>\n<td>spring.cloud.ans.cluster</td>\n<td>DEFAULT</td>\n<td>可以通过集群来分别标记服务</td>\n<td>test</td>\n</tr>\n<tr>\n<td>租户环境</td>\n<td>spring.cloud.ans.env</td>\n<td>DEFAULT</td>\n<td>相同租户的相同环境下的服务才能互相发现</td>\n<td>test</td>\n</tr>\n</tbody>\n</table>\n<h2>工作原理</h2>\n<p>下面我们将从 服务注册中心寻址、服务注册与下线、客户端结合、高可用、安全等多个方面来分析原理。</p>\n<h3>服务注册中心寻址</h3>\n<p>配置阿里云的 AccessKey 和 SecretKey 之后，再指定 EDAS 的命名空间，那么程序会在启动的过程中去调用 EDAS 的接口，获取到此命名空间对应的账号权限信息和地址服务器的地址。</p>\n<p>ANS 客户端通过地址服务器就能拿到 ANS Server端的地址。</p>\n<h3>服务注册与下线</h3>\n<p>服务注册的通信协议是 HTTP 协议，在 Spring 启动过程中，收到 EmbeddedServletContainerInitializedEvent 或 ServletWebServerInitializedEvent\n事件时，会将服务注册到 ANS Server 端。</p>\n<p>服务注册成功后，ANS client 端将会主动向 server 端发送心跳，当超过一定时间内 server 端没有收到 client 端的心跳时，会将服务标记成不可用，这样其他 client 在查询时就能发现此服务当前处于不可用的状态。<br>\n如果短时间内，大量 client 与 server 心跳失败，则会出发降级保护机制，服务会暂时不被标记成不可用的状态。</p>\n<p>当应用程序停止时，收到 ContextClosedEvent 事件后，ANS client 会调用 server 端的反注册接口，将此服务下线。</p>\n<h3>客户端结合</h3>\n<p>与客户端结合的方式， ans starter已经实现自动与 Ribbon 组件的结合。</p>\n<p>Ribbon的关键接口有下面三个:</p>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>提供的功能</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ServerList</td>\n<td>提供 getInitialListOfServers 和 getUpdatedListOfServers 方法，获取和刷新 serverList。</td>\n</tr>\n<tr>\n<td>IRule</td>\n<td>选择出一个服务节点</td>\n</tr>\n<tr>\n<td>IPing</td>\n<td>提供 isAlive 方法，负责探测serverList中节点是否可用</td>\n</tr>\n</tbody>\n</table>\n<p>目前 ans-starter 在 spring context 中默认实例化了一个 AnsServerList。AnsServerList 实现了 Ribbon 的 ServerList<Server> 接口。</p>\n<pre><code>@Bean\n@ConditionalOnMissingBean\npublic ServerList&lt;Server&gt; ribbonServerList(IClientConfig config) {\n    AnsServerList serverList = new AnsServerList(config.getClientName());\n    return serverList;\n}\n</code></pre>\n<p>如果您有其他需求，可以自行实现 Ribbon 的接口，或者直接在 github 上通过 issue 向我们提需求，我们评估通过后会加上此功能。</p>\n<h3>高可用实现</h3>\n<h4>服务端高可用</h4>\n<p>ANS作为分布式系统，服务端高可用包含以下几个方面：</p>\n<ul>\n<li>集群内机器数据对等，每台机器存储全量数据；</li>\n<li>支持机房容灾功能，即一个集群中的某个机房内的机器全部挂掉不影响服务；</li>\n<li>服务端异常自动探测，一旦某个机器挂了，该机器相关的任务（如健康检查）自动漂移到其他机器；</li>\n</ul>\n<h4>客户端高可用</h4>\n<p>ANS 为了保证高可用，在客户端高可用方面同样做了很多工作，以应对不同场景的异常情况。</p>\n<ul>\n<li>\n<p>推空保护，当客户端收到服务端推送的空数据时，忽略掉该数据，使用上次的数据；</p>\n</li>\n<li>\n<p>本地内存缓存，当运行时与服务注册中心的连接丢失或服务注册中心完全宕机，仍能正常地调用服务。</p>\n</li>\n<li>\n<p>本地缓存文件，当应用与服务注册中心发生网络分区或服务注册中心完全宕机后，应用进行了重启操作，内存里没有数据，此时应用可以通过读取本地已落盘持久化的数据来获取到最后一次订阅到的内容。</p>\n</li>\n<li>\n<p>本地容灾文件夹。正常的情况下，容灾文件夹内是没有内容的。当服务端完全宕机且长时间不能恢复，同时服务提供者又发生了很大的变更时，可以通过在容灾文件夹内添加文件的方式来开启本地容灾。此时客户端会忽略原有的本地缓存文件，只从本地容灾文件中读取配置。极端情况下，服务端完全不可用时，可以通过修改缓存文件的方式达到支持服务发布变更的目的。</p>\n</li>\n</ul>\n<h3>安全的实现</h3>\n<p>EDAS 服务注册发现组件，结合 EDAS 已有的安全功能，在每次注册、心跳和查询请求中都添加了验签鉴权的操作，保护了服务的安全性。</p>\n<p>所以如果您发现您配置的权限信息都无误，但是运行文档中的 Demo 却注册失败了。则可能是由于您本机的时间不准确，从而导致验签鉴权失败。此时您需要校正本机的时间，建议打开时间自动同步功能。</p>\n<h2>FAQ</h2>\n<h4>问：为什么我的服务注册总是失败？</h4>\n<h4>答：如果您在确认账号信息都准确无误的情况下，但是运行此文档中的 Demo 却注册失败了。有可能是由于您本机的时间不准确，从而导致验签鉴权失败。此时您需要校正本机的时间，建议打开时间自动同步功能。</h4>\n"
    },
    {
      "filename": "Nacos-is-Coming.md",
      "__html": "<h1>支持Dubbo生态发展，阿里巴巴启动新的开源项目 Nacos</h1>\n<h2>贡献Dubbo生态，阿里Nacos开源计划</h2>\n<p>在上周六的Aliware技术行上海站Dubbo开发者沙龙上，阿里巴巴高级技术专家郭平(坤宇)宣布了阿里巴巴的一个新开源计划，阿里巴巴计划在7月份开启一个名叫Nacos的新开源项目, 在活动演讲中，坤宇介绍了这个开源项目的初衷，他表示 “将通过Nacos项目将阿里巴巴在建设共享服务体系中使用的服务发现、配置及服务管理平台贡献给开源社区，通过打造Dubbo + Nacos的经典组合进一步释放Dubbo在云原生及Service Mesh时代中，在大规模微服务治理、流量治理、服务集成与服务共享等服务平台能力建设上的威力，同时Nacos会非常关注对主流开源社区，如Spring Cloud和Kubernetes云原生体系的无缝对接与支持”。该项目预计在7月中旬之前开放首个测试预览版本，并计划在未来6~8个月release的0.8版本开始达到生产可用的状态。</p>\n<p>活动的 <a href=\"http://www.itdks.com/dakashuo/playback/2307\">视频回放</a></p>\n<h2>什么是 Nacos /nɑ:kəʊs/?</h2>\n<p>Nacos 是阿里巴巴的新开源项目，其核心定位是 “一个更易于帮助构建云原生应用的动态服务发现、配置和服务管理平台”。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530083408486-b4698533-8e69-4e0a-9f3a-fdb1cdfbd290.png\" alt=\"Screen Shot 2018-06-27 at 15.09.35.png\"></p>\n<p>Nacos 有三大主要功能:</p>\n<ul>\n<li>服务发现与服务管理</li>\n</ul>\n<p>在采用以“服务(Service)”为中心的诸如微服务及云原生方式的现代应用架构时，动态服务发现至关重要。 Nacos同时支持基于DNS和基于RPC（如Dubbo/gRPC）的服务发现，并为您提供服务的实时的健康检查以防止将请求发送给不健康的主机，基于Nacos您也可以更方便的实现服务断路器。Nacos提供的强大的服务的元数据管理，路由及流量管理策略也能够帮助您更好的构建更强壮的微服务平台。</p>\n<ul>\n<li>动态配置管理</li>\n</ul>\n<p>动态配置服务允许您在所有环境中以集中和动态的方式管理所有应用程序或服务的配置。动态配置消除了配置更新时重新部署应用程序和服务的需要。可以更方便的帮助您实现无状态服务，更轻松地实现按需弹性扩展服务实例。</p>\n<ul>\n<li>动态DNS服务</li>\n</ul>\n<p>支持权重路由的动态DNS服务使您可以更轻松地在数据中心内的生产环境中实施中间层负载平衡，灵活的路由策略，流量控制和简单的DNS解析服务，帮助您更容易的实现DNS-based服务发现。</p>\n<h2>为什么开源 Nacos</h2>\n<p>阿里巴巴为什么选择这么一个时间点开源Nacos，其背后的思考是什么，坤宇也给出了详细的解读，在演讲中，坤宇表示主意基于以下几点:</p>\n<ul>\n<li><strong>围绕着Service为中心的分布式基础设施正在变的越来越重要</strong></li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077340492-513a25c0-51d0-494f-b39d-6f615d7e3915.png\" alt=\"Screen Shot 2018-06-27 at 13.28.09.png\"></p>\n<p>世界正在变的更快，创新和市场竞争的节奏正在变得愈发剧烈，如何超快速实现业务增长成为商业竞争的主旋律，几乎一夜之间共享单车就火遍全国，不到几年滴滴就改变了我们的打车方式，腾讯三班倒实现全民“吃鸡”，现在企业估值在从0到100亿所需的时间越来越短，而企业的平均寿命从标普的数据来看却从上世纪60年代的60年下降到了今天的15年，一切都表示创新和竞争的速度和烈度在加强。</p>\n<p>另一方面技术基础设施的敏捷和有效性在商业成功的要素上占据的比重越来越大，云计算在资源和服务交付模式上的变革，带来了效率的革命性提升，带来了更敏捷的基础设施（创业不用再买机器并找机房托管，从以前的半年准备周期到现在在云上的小时级创建全套服务），而在应用架构层面，微服务架构模式带来的灵活性、韧劲，快速组合和聚合原子服务从而创新，给业务快速创新和试错提供了条件，已经被越来越多的应用平台证明其有效性，技术基础设施的更敏捷，给商业的敏捷和商业的竞争优势提供了基础。</p>\n<p>在今天，无论是云计算，微服务还是围绕Kubernetes为中心的云原生，都在强调以“服务”为内核的应用架构模式，如果说15年前我们在讨论“一切皆是对象”构建单体系统，那么今天我们就是在谈论“一切皆是服务”，10年前淘宝服务化改造顺应了这种趋势，8年前微服务架构思想也顺应了这个趋势，今天面向“服务”的各种分布式基础设施正在变得越来越重要，站在阿里巴巴10年的服务化发展经验上看，在大规模服务发现和服务治理和服务共享领域现有的开源解决方案是不是都已经非常完美了呢？根据阿里巴巴服务化走过的这些年的生产经验来看，我们觉得并没有。</p>\n<ul>\n<li><strong>阿里巴巴在 &quot;共享服务体系&quot; 建设上的经验可以在各个行业大规模复用</strong></li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077414293-3d22cd01-6e3c-49d1-82ea-ad2f8d443cf9.png\" alt=\"Screen Shot 2018-06-27 at 13.29.52.png\"></p>\n<p>阿里巴巴中台理念和体系，与云原生在精神的“道”上完全契合，即“厚技术平台，薄应用” 支持业务的快速创新与试错，从而赢得市场，中台体系倡导双引擎架构，略过“大数据”不谈，但看业务中台，就是一个大的以“服务”为中心的共享服务平台，在线服务沉淀业务数据，同步到大数据平台计算和挖掘，大数据平台则通过数据回馈，指导业务及服务的创新，支成可沉淀和可共享“服务”体系的服务注册与服务治理平台是这个体系的关键要素之一。</p>\n<ul>\n<li><strong>“服务治理，服务沉淀、服务共享和服务的可持续发展”是“共享服务体系”的核心价值主张</strong></li>\n</ul>\n<p>支持创新从小苗长成参天大树，服务平台不断演进，这一切需要一个强大的服务平台和服务基础设施的支撑。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077444757-d36f344d-d3ae-4df0-808f-8500b4320e8d.png\" alt=\"Screen Shot 2018-06-27 at 13.30.25.png\"></p>\n<ul>\n<li><strong>阿里巴巴将通过Dubbo + Nacos以及一系列开源项目打造服务发现、服务及流量管理、服务共享平台</strong></li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077476468-4ce30dd9-0da7-414a-875d-acc151739c1d.png\" alt=\"Screen Shot 2018-06-27 at 13.30.58.png\"></p>\n<h2>Nacos 与 主流开源生态的关系</h2>\n<p>Nacos 不会是个封闭的体系，除了对于阿里开源生态体系如Dubbo等自身的支持，也非常强调融入其它的开源生态，这里就包括Java的微服务生态体系Spring Cloud，Kubernetes/CNCF云原生生态体系，正如Nacos的未来全景图展示的那样</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077509217-d4fec359-8a41-4c51-a986-c44e06d16950.png\" alt=\"Screen Shot 2018-06-27 at 13.31.30.png\"></p>\n<ul>\n<li>Dubbo + Nacos， 专为Dubbo而生的注册中心与配置中心</li>\n</ul>\n<p>在阿里巴巴生产环境上，Dubbo和Nacos天然就是长在一起的，因为Nacos的缺失，传统的注册中心解决方案让Dubbo在服务治理、流量治理、服务运营和管理等方面的威力被限制和削弱了，Nacos的开源和开放会在采用Dubbo的用户环境中释放这些威力</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077613925-04d767fd-ec95-4fe2-8249-ce8650fbe372.png\" alt=\"Screen Shot 2018-06-27 at 13.31.57.png\"></p>\n<ul>\n<li>Nacos 会完全兼容Spring Cloud</li>\n</ul>\n<p>Nacos会无缝支持Spring Cloud，为Spring Cloud用户其提供更简便的配置中心和注册中心的解决方案，使用Nacos不用再仅仅为服务和配置就需要在生产上hold住 Eureka，Spring Cloud Config Server，Git，RabbitMQ/Kafka 起码四个开源产品。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077646935-7ca8a74d-59e5-4e01-865e-06fd5631e623.png\" alt=\"Screen Shot 2018-06-27 at 13.33.40.png\"></p>\n<ul>\n<li>Nacos 支持Kubernetes DNS-based Service Discovery</li>\n</ul>\n<p>在演讲中坤宇也表示，阿里巴巴这么多年在VIPServer DNS-based Service Discovery上的实践证明，在云原生时代，应用会更关注与基础设施的解耦合、多语言乃至多云的诉求，服务发现的未来一定是基于标准的DNS协议做，而不是像Eureka或者像ZooKeeper这样的私有API或者协议做, 同时在云上，在服务发现场景中，注册中心更关注的是可用性而不是数据一致性，所以Nacos会首推DNS-based Servcie Discovery，并优先关注可用性，而这也正是Nacos可以无缝融合进Kubernetes服务发现体系的原因所在</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077691493-4dae9605-6886-4307-bc9d-16ba1cceb7ac.png\" alt=\"Screen Shot 2018-06-27 at 13.34.18.png\"></p>\n<ul>\n<li>Nacos 会填补Spring Cloud 体系与 Kubernetes 体系的鸿沟</li>\n</ul>\n<p>未来会有越来越多java生态的用户会选择 Kubernetes+Spring Cloud 组合，但不幸的是，在服务发现和配置管理的解决方案上，这2个体系都采用了完全不同的方案，这给同时采用2个体系的用户在注册中心和配置中心的需求上带来了非常大的不必要的复杂性。Nacos会尝试填补2者的鸿沟，以便在2套体系下可以采用同一套服务发现和配置管理的解决方案，这将大大的简化使用和维护的成本。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077731231-85a58c0e-ff5c-45a1-9bc8-b989e2b09d55.png\" alt=\"Screen Shot 2018-06-27 at 13.35.05.png\"></p>\n<ul>\n<li>Nacos 与 Service Mesh</li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077754298-39bbbdd0-c7af-434a-b189-8fd7ac958b43.png\" alt=\"Screen Shot 2018-06-27 at 13.35.38.png\"></p>\n<h2>Nacos 部分特性预览</h2>\n<p>同时在会上，坤宇对Nacos 1.0版本的部分特性给大家做了预览</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077784470-27f0e268-8d1f-443a-b1a5-3d59e15ab8f4.png\" alt=\"Screen Shot 2018-06-27 at 13.36.06.png\"></p>\n<p>每个产品都有自己的风格和标签，坤宇在演讲中表示，团队会通过持续的贡献和努力，希望未来给Nacos贴上四个方面的标签</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077851626-25444089-c027-4b89-a606-2e693b4e052d.png\" alt=\"Screen Shot 2018-06-27 at 13.37.10.png\"></p>\n<p>在部署形态上，Nacos会支持多种部署形态，包括注册中心与配置中心的分离部署，同时在阿里云上提供Nacos的SaaS化部署服务，感兴趣的可以直接在阿里云上开通账户免费体验Nacos服务，在开源与商业化版本差别上，商业化的ACM以及EDAS ANS更强调与阿里云其它云服务以及其它Aliware PaaS的商业产品的集成体验以及提供商业服务。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077875162-26cfa0e2-3da0-473e-98b9-190aa6d6d97a.png\" alt=\"Screen Shot 2018-06-27 at 13.36.37.png\"></p>\n<h2>Nacos 的主要产品里程碑及计划</h2>\n<blockquote>\n<p>罗马不是一天建成的，吾将上下而求索</p>\n</blockquote>\n<p>因为Nacos是脱胎于阿里巴巴的生产代码，整体体系非常庞杂，在代码梳理、重构和剥离与内部的耦合上是一个渐进的过程，所以@坤宇特别提醒社区在Nacos 0.8达到生产可用状态前，不建议用于生产，不过可以在开发和测试环境尝试使用，在0.8版本开始，大家可以放心的用于生产环境，Nacos整体研发计划是在未来6-8个月将达到生产可用的状态，未来会很快启动将Nacos贡献给开源基金会进一步社区化发展。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077912860-52c7fea4-5285-45a0-a3b8-73ba010c53da.png\" alt=\"Screen Shot 2018-06-27 at 13.38.13.png\"></p>\n<h2>Nacos 与相关开源产品的对比</h2>\n<blockquote>\n<p>君子和而不同</p>\n</blockquote>\n<p>如上面对整体业务及技术形式的判断，我们可以看到 Nacos 与同类竞品的主要不同主要在于理念，@坤宇介绍说，严格来说这些市面上的产品并不与Nacos完全对标，只是与Nacos里面的服务发现部分对标，Nacos的未来更看重在这些产品的基础上构建服务平台的能力，而不仅仅是在基础的服务发现能力上。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077938157-00e6b34f-9a16-498c-9b89-cc48f6d6bb73.png\" alt=\"Screen Shot 2018-06-27 at 13.38.41.png\"></p>\n<h2>社区化发展，欢迎加入并贡献社区</h2>\n<blockquote>\n<p>DISS is cheap, show me your hand\n比吐槽更重要的是搭把手，参与社区一起发展Nacos</p>\n</blockquote>\n<p>与阿里巴巴早期的开源不同，阿里巴巴新一轮的开源包括RocketMQ，Pouch Container，Dubbo, Nacos等开源产品更强调社区化的发展与社区的多样性，鼓励更多的公司和更多的开发者参与到开源项目中来，依托于社区将产品做得更好，同时一开始就会关注国际化，与国外同类产品的直面竞争。</p>\n<p>Nacos初步计划，在第一年就吸收至少超过5名来自其它公司的PMC,至少10名的外部Committer, 而且Nacos处在项目开源的初期，有大把的空间让有想法、有热情、有能力的开发者参与进来，Nacos本身在很多方面都急需要社区的帮助，这里面就急需包括前端及UI建设，Spring Cloud、Kubernetes、Service Mesh体系融合与集成，多语言客户端等各方面的技术领导者的参与，如果您对Nacos这个开源项目感兴趣，欢迎加入Nacos社区。你可以通过扫“超哥”的微信二维码，让“超哥” 帮你加入 “Nacos社区交流群”</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077965587-8f4e3100-bdd4-469a-9ea0-7af7061bc9ef.png\" alt=\"Screen Shot 2018-06-27 at 13.39.09.png\"></p>\n<h2>感谢所有未来给我们帮助的朋友</h2>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530078016801-65ce72c1-c135-42b6-8076-a046f7829ba7.png\" alt=\"Screen Shot 2018-06-27 at 13.39.40.png\"></p>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://yq.aliyun.com/articles/601745\">阿里巴巴为什么不用 ZooKeeper 做服务发现？</a></li>\n<li><a href=\"https://yq.aliyun.com/articles/226238?spm=5176.163362.847322.3.5a8925397in790\">现代应用架构中的配置管理面临的挑战和应对之道</a></li>\n<li><a href=\"http://jm.taobao.org/2016/06/02/zhangwensong-and-load-balance/\">章文嵩博士和他背后的负载均衡帝国</a></li>\n<li><a href=\"https://blog.csdn.net/heyc861221/article/details/80126013\">VIPServer：阿里智能地址映射及环境管理系统详解</a></li>\n</ul>\n"
    },
    {
      "filename": "alibaba-configserver.md",
      "__html": "<h1>《阿里巴巴服务注册中心产品ConfigServer 10年技术发展回顾》</h1>\n<p>作者：阿里巴巴高级技术专家，许真恩(慕义)</p>\n<p>文章概要：本文简单描述了Eureka1.0存在的架构问题，Eureka2.0设想的架构。详细回顾了阿里巴巴的服务注册中心ConfigServer产品从2008年建设元年至今经历的关键架构演进。通过这个文章你会对基于AP模式的注册中心在技术发展过程中将会碰到的问题有所感知。</p>\n<h1>Eureka1.0架构存在的问题</h1>\n<p>Eureka作为Netflix公司力推和SpringCloud微服务标配的注册中心开源解决方案，其<span data-type=\"color\" style=\"color:rgb(36, 41, 46)\">Eureka 2.0 (Discontinued)的消息在社区引起了不小的骚动；其实早在2015年社区就已经放出了2.0架构的升级设想，但是3年的时间过去，等到的确是Discontinued的消息，虽然2.0的代码在github的主页上也已经放出，但是告诫用户要自行承担当中的使用风险。我想不会有人真的把2.0直接投入到生产中使用。</span></p>\n<p>对于为什么要做Eureka2.0，其官方的wiki中的Why Eureka 2.0和Eureka 2.0 Improvements做了如下的说明</p>\n<ul>\n<li>Only support homogenous client views: Eureka servers only expects the client to always get the entire registry and does not allow to get specific applications/VIP addresses. This imposes a memory limitation on all clients registering with Eureka, even if they need a very small subset of the Eureka’s registry.</li>\n<li>Only supports scheduled updates: Eureka client follows a poll model to fetch updates from the server routinely. This imposes an overhead on the client of doing a scheduled call to the server, even if there are no changes and also delays the updates by the poll interval.</li>\n<li>Replication algorithm limits scalability: Eureka follows a broadcast replication model i.e. all the servers replicate data and heartbeats to all the peers. This is simple and effective for the data set that eureka contains however replication is implemented by relaying all the HTTP calls that a server receives as is to all the peers. This limits scalability as every node has to withstand the entire write load on eureka.</li>\n</ul>\n<p>Although Eureka 2.0 provides a more richer feature set, the above limitations are the major driving factors for the changes proposed in this version.\nBased on the above motivations, Eureka 2.0 achieves the following improvements:</p>\n<ul>\n<li>Interest based subscription model for registry data: A client of Eureka is able to select a part of the instance registry in which it is interested in and the eureka server only sends information about that part of the registry. Eg: A client can say I am only interested in application “WebFarm” and then the server will only send information about WebFarm instances. Eureka server provides various selection criterions and a way to update this interest set dynamically.</li>\n<li>Push model from the server for any changes to the interest set: Instead of the current pull model from the client, Eureka servers pushes updates for changes to the interest set, to the client.</li>\n<li>Optimized replication: As Eureka 1.0, Eureka 2.0 also follows a broadcast replication model i.e. every node replicates data to all other nodes. However, the replication algorithm is much more optimized eliminating the need of sending heartbeats for every instance in the registry. This drastically reduce the replication traffic and achieves much higher scalability.</li>\n<li>Auto-scaled Eureka servers: Eureka 2.0 divides the read (discovery of data) and write (registration) concerns into separate clusters. Since, the write load is predictable (proportional to the number of instances in a region), the write cluster is pre-scaled. On the other hand, read load is unpredictable (proportional to subscriptions from clients) and hence the read farm is auto-scaled.</li>\n<li>Audit log: Eureka 2.0 provides an elaborate audit log for any change done to the registry. This helps Eureka owners as well as users to get insight into debugging the state of individual application instances as exists in Eureka. The audit log by default is persisted to a log file, but different persistent storages can be plugged-in.</li>\n<li>Dashboard: Eureka 2.0 provides a rich dashboard (as opposed to very rudimentary dashboard of Eureka 1.0) with insights into Eureka internals with respect to registry views, server health, subscription state, audit log, etc.</li>\n</ul>\n<p>简单翻译总结，也就是Eureka1.0的架构主要存在如下的不足：</p>\n<ul>\n<li>订阅端拿到的是服务的全量的地址：这个对于客户端的内存是一个比较大的消耗，特别在多数据中心部署的情况下，某个数据中心的订阅端往往只需要同数据中心的服务提供端即可。</li>\n<li>pull模式：客户端采用周期性向服务端主动pull服务数据的模式（也就是客户端轮训的方式），这个方式存在实时性不足以及无谓的拉取性能消耗的问题。</li>\n<li>一致性协议：Eureka集群的多副本的一致性协议采用类似“异步多写”的AP协议，每一个server都会把本地接收到的写请求（register/heartbeat/unregister/update）发送给组成集群的其他所有的机器（Eureka称之为peer），特别是hearbeat报文是周期性持续不断的在client-&gt;server-&gt;all peers之间传送；这样的一致性算法，导致了如下问题\n<ul>\n<li>每一台Server都需要存储全量的服务数据，Server的内存明显会成为瓶颈。</li>\n<li>当订阅者却来越多的时候，需要扩容Eureka集群来提高读的能力，但是扩容的同时会导致每台server需要承担更多的写请求，扩容的效果不明显。</li>\n<li>组成Eureka集群的所有server都需要采用相同的物理配置，并且只能通过不断的提高配置来容纳更多的服务数据</li>\n</ul>\n</li>\n</ul>\n<p>Eureka2.0主要就是为了解决上述问题而提出的，主要包含了如下的改进和增强</p>\n<ul>\n<li>数据推送从pull走向push模式，并且实现更小粒度的服务地址按需订阅的功能。</li>\n<li>读写分离：写集群相对稳定，无需经常扩容；读集群可以按需扩容以提高数据推送能力。</li>\n<li>新增审计日志的功能和功能更丰富的Dashboard。</li>\n</ul>\n<p>Eureka1.0版本存在的问题以及Eureka2.0架构设想和阿里巴巴内部的同类产品ConfigServer所经历的阶段非常相似（甚至Eureka2.0如果真的落地后存在的问题，阿里巴巴早已经发现并且已经解决），下面我带着你来回顾一下我们所经历过的。</p>\n<h1>阿里巴巴服务注册中心ConfigServer技术发现路线</h1>\n<p>阿里巴巴早在2008就开始了服务化的进程，在那个时候就就已经自研出服务发现解决方案（内部产品名叫ConfigServer）。</p>\n<p>当2012年9月1号Eureka放出第一个1.1.2版本的时候，我们把ConfigServer和Eureka进行了深度的对比，希望能够从Eureka找到一些借鉴来解决当时的ConfigServer发展过程中碰到的问题（后面会提到）；然而事与愿违，我们已经发现Eureka1.x架构存在比较严重的扩展性和实时性的问题（正如上面所描述的），这些问题ConfigServer当时的版本也大同小异的存在，我们在2012年底对ConfigServer的架构进行了升级来解决这些问题。</p>\n<p>当2015年Eureka社区放出2.0架构升级的声音的时候，我们同样第一时间查看了2.0的目标架构设计，我们惊奇的发现Eureka的这个新的架构同2012年底ConfigServer的架构非常相似，当时一方面感慨“英雄所见略同”，另一方也有些失望，因为ConfigServer2012年的架构早就无法满足阿里巴巴内部的发展诉求了。</p>\n<p>下面我从头回顾一下，阿里巴巴的ConfigServer的技术发展过程中的几个里程碑事件。</p>\n<h2>2008年，无ConfigServer的时代</h2>\n<p>借助用硬件负载设备F5提供的vip功能，服务提供方只提供vip和域名信息，调用方通过域名方式调用，所有请求和流量都走F5设备。</p>\n<p>遇到的问题：</p>\n<ul>\n<li>负载不均衡：对于长连接场景，F5不能提供较好的负载均衡能力。如服务提供方发布的场景，最后一批发布的机器，基本上不能被分配到流量。需要在发布完成后，PE手工去断开所有连接，重新触发连接级别的负载均衡。</li>\n<li>流量瓶颈：所有的请求都走F5设备，共享资源，流量很容易会打满网卡，会造成应用之间的相互影响。</li>\n<li>单点故障：F5设备故障之后，所有远程调用会被终止，导致大面积瘫痪。</li>\n</ul>\n<h2>2008年，ConfigServer单机版V1.0</h2>\n<p>单机版定义和实现了服务发现的关键的模型设计（包括服务发布，服务订阅，健康检查，数据变更主动推送，这个模型至今仍然适用），应用通过内嵌SDK的方式接入实现服务的发布和订阅；这个版本很重要的一个设计决策就是服务数据变更到底是pull还是push的模式，我们从最初就确定必须采用push的模式来保证数据变更时的推送实时性（Eureka1.x的架构采用的是pull的模式）</p>\n<p>当时，HSF和Notify就是采用单机版的ConfigServer来完成服务的发现和topic的发现。单机版的ConfigServer和HSF、Notify配合，采用服务发现的技术，让请求通过端到端的方式流动，避免产生流量瓶颈和单点故障。ConfigServer可以动态的将服务地址推送到客户端，服务调用者可以知道所有提供此服务的机器，每个请求都可以通过随机或者轮询的方式选择服务端，做到请求级别的负载均衡。这个版本已经能很好的解决F5设备不能解决的三个问题。</p>\n<p>但是单机版本的问题也非常明显，不具备良好的容灾性；</p>\n<h2>2009年初，ConfigServer单机版V1.5</h2>\n<p>单机版的ConfigServer所面临的问题就是当ConfigServer在发布/升级的时候，如果应用刚好也在发布，这个时候会导致订阅客户端拿不到服务地址的数据，影响服务的调用；所以当时我们在SDK中加入了本地的磁盘缓存的功能，应用在拿到服务端推送的数据的时候，会先写入本地磁盘，然后再更新内存数据；当应用重启的时候，优先从本地磁盘获取服务数据；通过这样的方式解耦了ConfigServer和应用发布的互相依赖；这个方式沿用至今。（我很惊讶，Eureka1.x的版本至今仍然没有实现客户端磁盘缓存的功能，难道Eureka集群可以保持100%的SLA吗？）</p>\n<h2>2009年7月，ConfigServer集群版本V2.0</h2>\n<p>ConfigServer的集群版本跟普通的应用有一些区别：普通的应用通过服务拆分后，已经属于无状态型，本身已经具备良好的可扩展性，单机变集群只是代码多部署几台；ConfigServer是有状态的，内存中的服务信息就是数据状态，如果要支持集群部署，这些数据要不做分片，要不做全量同步；由于分片的方案并没有真正解决数据高可用的问题（分片方案还需要考虑对应的failover方案），同时复杂度较高；所以当时我们选择了单机存储全量服务数据全量的方案。为了简化数据同步的逻辑，服务端使用客户端模式同步：服务端收到客户端请求后，通过客户端的方式将此请求转发给集群中的其他的ConfigServer，做到同步的效果，每一台ConfigServer都有全量的服务数据。</p>\n<p>这个架构同Eureka1.x的架构惊人的相似，所以很明显的Eureka1.x存在的问题我们也存在；当时的缓解的办法是我们的ConfigServer集群全部采用高配物理来部署。</p>\n<h2>2010年底，ConfigServer集群版V2.5</h2>\n<p>基于客户端模式在集群间同步服务数据的方案渐渐无以为继了，特别是每次应用在发布的时候产生了大量的服务发布和数据推送，服务器的网卡经常被打满，同时cmsgc也非常频繁；我们对数据同步的方案进行了升级，去除了基于客户端的同步模式，采用了批量的基于长连接级别的数据同步+周期性的renew的方案来保证数据的一致性（这个同步方案当时还申请了国家专利）；这个版本还对cpu和内存做了重点优化，包括同步任务的合并，推送任务的合并，推送数据的压缩，优化数据结构等；</p>\n<p>这个版本是ConfigServer历史上一个比较稳定的里程碑版本。</p>\n<p><span data-type=\"color\" style=\"color:rgb(36, 41, 46)\">但是随着2009年天猫独创的双十一大促活动横空出世，服务数量剧增，应用发</span>布时候ConfigServer集群又开始了大面积的抖动，还是体现在内存和网卡的吃紧，甚至渐渐到了fullgc的边缘；为了提高数据推送能力，需要对集群进行扩容，但是扩容的同时又会导致每台服务器的写能力下降，我们的架构到了“按下葫芦浮起瓢”的瓶颈。</p>\n<h2>2012年底，ConfigServer集群版V3.0</h2>\n<p>在2011年双十一之前我们完成了V3架构的落地，类似Eureka2.0所设计的读写分离的方案，我们把ConfigServer集群拆分成session和data两个集群，客户端分片的把服务数据注册到session集群中，session集群会把数据异步的写到data集群，data集群完成服务数据的聚合后，把压缩好的服务数据推送到session层缓存下来，客户端可以直接从session层订阅到所需要的服务数据；这个分层架构中，session是分片存储部分的服务数据的（我们设计了failover方案），data存储的是全量服务数据（天生具备failover能力）；</p>\n<p>data集群相对比较稳定，不需要经常扩容；session集群可以根据数据推送的需求很方便的扩容（这个思路和Eureka2.0所描述的思路是一致的）；session的扩容不会给data集群带来压力的增加。session集群我们采用低配的虚拟机即可满足需求，data由于存储是全量的数据我们仍然采用了相对高配的物理机（但是同V2.5相比，对物理机的性能要求已经答复下降）</p>\n<p>这个版本也是ConfigServer历史上一个划时代的稳定的大版本。</p>\n<h2>2014年，ConfigServer集群版V3.5</h2>\n<p>2013年，阿里巴巴开始落地了异地多活方案，从一个IDC渐渐往多个IDC跨越，随之而来的对流量的精细化管控要求越来越高（比如服务的同机房调用，服务流量的调拨以支持灰度能力等），基于这个背景ConfigServer引入了服务元数据的概念，支持对服务和IP进行元数据的打标来满足各种各样的服务分组诉求。</p>\n<h2>2017年，ConfigServer集群版V4.0</h2>\n<p>V3版本可见的一个架构的问题就是data集群是存储全量的服务数据的，这个随着服务数的与日俱增一定会走到升级物理机配置也无法解决问题的情况（我们当时已经在生产使用了G1的垃圾回收算法），所以我们继续对架构进行升级，基于V3的架构进行升级其实并不复杂；session层的设计保持不变，我们把data进行分片，每一个分片同样按照集群的方式部署以支持failover的能力；</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ConfigServer</th>\n<th>Eureka</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2008年</td>\n<td>V1.0：单机版，定义了服务发现的领域模型</td>\n<td></td>\n</tr>\n<tr>\n<td>2009年初</td>\n<td>V1.5：应用和ConfigServer集群发布解耦</td>\n<td></td>\n</tr>\n<tr>\n<td>2009年7月</td>\n<td>V2.0：基于客户端模式同步数据，支持集群部署</td>\n<td></td>\n</tr>\n<tr>\n<td>2010年底</td>\n<td>V2.5：优化集群间数据同步模式，申请国家专利。</td>\n<td></td>\n</tr>\n<tr>\n<td>2012年9月1号</td>\n<td></td>\n<td>Eureka1.0正式开源</td>\n</tr>\n<tr>\n<td>2012年底</td>\n<td>V3.0：支持session和data分层部署</td>\n<td></td>\n</tr>\n<tr>\n<td>2014年</td>\n<td>V3.5：支持异地多活等细分场景</td>\n<td></td>\n</tr>\n<tr>\n<td>2015年</td>\n<td></td>\n<td>Eureka2.0架构升级方案公布</td>\n</tr>\n<tr>\n<td>2017年</td>\n<td>V4.0：支持data分片能力</td>\n<td></td>\n</tr>\n<tr>\n<td>2018年7月</td>\n<td></td>\n<td>Eureka2.0架构升级宣布停止</td>\n</tr>\n</tbody>\n</table>\n<h1>Nacos</h1>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">作为同属于AP类型的注册中心，Eureka和ConfigServer发展过程中所面临的诸多问题有很大的相似性，但是阿里巴巴这些年业务的跨越式发展，每年翻番的服务规模，不断的给ConfigServer的技术架构演进带来更高的要求和挑战，我们有更多的机会在生产环境发现和解决一个个问题的过程中，做架构的一代代升级。我们正在计划通过开源的手段把我们这些年在生产环境上的实践经验通过Nacos(</span></span><a href=\"http://nacos.io\">链接</a>)<span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">产品贡献给社区，一方面能够助力和满足同行们在微服务落地过程当中对工业级注册中心的诉求，另一方面也希望通过开源社区及开源生态的协同发展给ConfigServer带来更多的可能性。</span></span></p>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">炎炎夏日，在Eureka 2.0 (Discontinued) 即将结束的时候，在同样的云原生时代，Nacos却正在迎来新生，技术演进和变迁的趣味莫过于此。</span></span></p>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">Nacos将努力继承Eureka未竟的遗志，扛着AP系注册中心的旗帜继续前行，不同的是Nacos更关注DNS-based Service Discovery以及与Kubernetes体系的融会贯通。</span></span></p>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">我们看不透未来，却仍将与同行们一起上下求索，砥砺前行。</span></span></p>\n<p>最后附上Nacos的架构图。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/12425/1530856061648-7180b97f-b61d-4127-924e-a0743b9e9d5a.png\" alt=\"屏幕快照 2018-06-28 下午4.58.59.png | center | 748x544\"></p>\n"
    },
    {
      "filename": "nacos.md",
      "__html": "<h1>Nacos 0.1.0 �汾Review ����</h1>\n<h2>I. ������б�</h2>\n<ul>\n<li>�Ķ������İ����ҹ���bug, ���С�Ӣ�Ĺ����Ὠ���Խ���</li>\n<li>�Ķ���Ӣ�ĵ������ĵ�bug, ���С�Ӣ�ĵ������Ὠ�飨�����ǹ�עӢ�ķ��벻�õĵط�����ΪӢ�Ķ������ǳ���Ա�Լ�ߣ�ģ�</li>\n<li>�����´��� -&gt;������ -&gt; ��Nacos server -&gt; ֹͣNacos server���̣�����Ľ����</li>\n<li>���������Լ����ڵ� Nacos ��Ⱥģʽ������Ľ����</li>\n<li>����ʹ��Nacos Java SDK, ��Java SDK��Ľ�����</li>\n<li>����ʹ��Nacos Open API����OpenAPI��Ľ�����</li>\n<li>���Ը��ݡ���ι���Naocs�ĵ� TODO����һ�� �������̣��������������Ὠ��</li>\n<li>��Nacos�����󡢷�չ�ƻ����뷨��Ҫ���</li>\n</ul>\n<h2>II. ����뷽ʽ</h2>\n<ul>\n<li>\n<p>ɨ�� �����硱 ΢��2΢�룬�ó���������� ��Nacos��������Ⱥ��</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/11189/1532004866850-5e03b901-6d76-4380-b7bf-66e227808bdc.png\" alt=\"΢�Ŷ�ά��\"></p>\n</li>\n<li>\n<p>ѡ��I �е�һ�����߶����������</p>\n</li>\n<li>\n<p>�����������BUG֮�󣬰���III�еġ�����Report��ʽ������һ����Ӧ�� github issue, ��ָ�ɸ� @ github�˺�<a href=\"https://github.com/xuechaos\">xuechaos</a></p>\n</li>\n</ul>\n<h2>III. ����Report��ʽ</h2>\n<ul>\n<li>����˵�� TODO</li>\n</ul>\n<h2>IV. ������ʽ</h2>\n<ul>\n<li>��������Ϊ���벢����ͻ�����׵�С��鶨��һЩС��Ʒ���ῼ�ǿ�ݸ���������ͻ�����׵�С���</li>\n<li>�������ᣬ��ϣ���ܱ������ǵİ����ĸм�֮�����һ</li>\n</ul>\n<h2>V. ����˵��</h2>\n<ul>\n<li>���ǲ�ȷ��ÿ��������󶼻ᱻ���ã��������Ǿ����������ͨ���ǻ��ں��ֿ��ǣ����Ľ����������û�в���</li>\n<li>����ͨ���ʼ��б����report issue�ķ�ʽ����������΢��Ⱥ��report���⣬�Ա㽫���ǵĹ�ͨ�����ĵ����͸����׳���</li>\n</ul>\n"
    }
  ],
  "en-us": [
    {
      "filename": "DNS-SD.md",
      "__html": "<p>当前，微服务架构已经成为企业尤其是互联网企业技术选型的一个重要参考。微服务架构中涉及到很多模块，本文将重点介绍微服务架构的服务注册与发现以及如何基于DNS做服务发现。最后，简单介绍下阿里巴巴内部是如何基于DNS做服务发现的。</p>\n<h2>服务注册与发现</h2>\n<p>微服务体系中，服务注册与服务发现是两个最核心的模块。服务A调用服务B时，需要通过服务发现模块找到服务B的IP和端口列表，而服务B的实例在启动时需要把提供服务的IP和端口注册到服务注册中心。一个典型的结构如下图：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185623334-8fbf69bb-00df-4202-a951-c1ee9caace88.png\" alt=\"image.png | left | 733x470\"></p>\n<h3>服务注册</h3>\n<p>目前，流行的注册中心比较多，常见的有zookeeper、ectd、consul、eureka等。服务注册通常有三种：自注册、第三方注册、注册中心主动同步。</p>\n<ul>\n<li>自注册\n自注册，顾名思义，就是服务提供方在启动服务时自己把提供服务的IP和端口发送到注册中心，并通过心跳方式维持健康状态；服务下线时，自己把相应的数据删除。典型的像使用eureka客户端发布微服务。</li>\n<li>第三方注册\n第三方注册是指，存在一个第三方的系统负责在服务启动或停止时向注册中心增加或删除服务数据。典型的用法是devops系统或容器调度系统主动调注册中心接口注册服务；</li>\n<li>注册中心主动同步\n与第三方注册方式类似，主动注册方式是指注册中心和调度或发布系统打通，主动同步最新的服务IP列表；一个例子是kubernetes体系中，coredns订阅api server数据。</li>\n</ul>\n<h3>服务发现</h3>\n<p>在真正发起服务调用前，调用方需要从注册中心拿到相应服务可用的IP和端口列表，即服务发现。服务发现从对应用的侵入性上可以分为两大类：</p>\n<ul>\n<li>SDK-Based\n这类的服务发现方式，需要调用方依赖相应的SDK，显式调用SDK代码才可以实现服务调用，即对业务有侵入性，典型例子如eureka、zookeeper等。</li>\n<li>DNS-Based\nDNS本身是一种域名解析系统，可以满足简单的服务发现场景，如双方约定好端口、序列化协议等等。但是，这远远不能满足真正的微服务场景需求。近几年，基于dns的服务发现渐渐被业界提了出来。后面将重点介绍基于DNS的服务发现及阿里巴巴的实践。</li>\n</ul>\n<h2>基于DNS的服务发现</h2>\n<p>DNS协议是目前最为通用的协议之一，几乎所有操作系统都会有DNS客户端实现。所以，其天然具有跨语言特性。这也是快速接入微服务体系最快的一个方式。要基于DNS做服务发现，首先注册中心的数据应该可以以DNS的数据格式暴露出来。让任何系统的DNS 客户端都可以通过DNS协议获取服务列表。</p>\n<p>基于DNS的服务发现方式，大致可以归结两类：</p>\n<h3>独立DNS服务器</h3>\n<p>独立DNS Server模式的基本架构如下图：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185653048-094f815d-47af-409e-ac21-6662e20e76f4.png\" alt=\"image.png | left | 747x469\"></p>\n<p>如上图所示，这种架构中，需要独立的DNS服务器。DNS服务器从注册中心获取所有已注册的服务及对应的IP端口列表。调用方Service A 通过DNS查询某个服务下的IP列表，然后发起调用。</p>\n<p>这种类型的服务发现方式优缺点分别如下：</p>\n<ul>\n<li>优点\n<ul>\n<li>集中的DNS服务器，便于升级维护</li>\n</ul>\n</li>\n<li>缺点\n<ul>\n<li>对DNS服务器性能要求高</li>\n<li>这种情况下一般需要LVS设备为DNS服务器集群做请求转发，存在单点问题</li>\n</ul>\n</li>\n</ul>\n<h3>DNS Filter</h3>\n<p>DNS Filter模式我们定义为把DNS服务器集成到服务调用方机器或容器里，如下图所示：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185676597-acaebac3-2b65-48a3-bdbf-7d27af7f1038.png\" alt=\"image.png | left | 747x469\"></p>\n<p>这种模式中，首先要保证ServiceA的DNS查询都被拦截到本机的DNS服务器上（127.0.0.1:53），在获取到服务的IP列表后发起调用。由于这种方式把DNS服务器前置到实际调用的机器上，所以它解决了独立DNS服务器模式的单点问题，完全P2P的模式。但由于需要在应用机器上安装DNS服务器，其维护和升级成本较前者高一些。</p>\n<h3>阿里基于DNS的服务发现实践</h3>\n<p>阿里巴巴早在2014年就开始了基于DNS做服务发现的尝试了，现在已经形成了较为成熟的模式。阿里内部以VIPServer作为注册中心，并开发了DNS Filter，部署在应用容器内。目前已经有超过20w个机器或容器上安装了DNS Filter，支持了几乎所有REST服务发现。</p>\n<h4>注册中心 VIPServer</h4>\n<p>VIPServer是阿里中间件软负载团队自研的服务注册中心，按照第一章的分类，VIPServer同时支持三种模式的服务注册，并且均有相当规模的应用。除此之外，VIPServer具备如下特性：</p>\n<ul>\n<li>主动与被动健康检查\nVIPServer同时支持主动与被动健康检查。主动健康检查是指VIPserver服务端主动定期发送健康检查探测包，探测服务提供方是否可以正常提供服务。用户可以配置多种健康检查方式，自定义探测端口和探测URL（HTTP）。主动探测的好处在于服务提供方不用做任何改动即可快融入微服务架构。被动健康检查则是指服务提供方主动注册自己的IP、端口和服务名等信息，并通过心跳方式保持活性。</li>\n<li>多种负载均衡策略\n同时，VIPServer支持多种负载均衡策略，包括权重、同机房、同地域、同环境等等，是阿里异地多活项目的核心系统之一。后面会有专门的文章介绍如何基于VIPServer实现异地多活，这里不再赘述。</li>\n<li>多重容灾保护策略\nVIPServer提供了多种容灾保护策略，可以有效减少人为或者底层系统（网络等）异常带来的影响。这些容灾保护包括：\n<ul>\n<li>客户端缓存，即使VIPServer服务端挂掉也不影响应用的正常调用；</li>\n<li>服务端保护阈值，有效防止应用因压力过大而发生雪崩；</li>\n<li>客户端容灾目录，提供人工干预服务IP列表的能力；</li>\n<li>客户端空列表保护，有效防止人为误删IP列表操作或VIPServer服务端异常</li>\n</ul>\n</li>\n</ul>\n<h4>DNS-F客户端</h4>\n<p>出于性能的考虑，我们采用了第二种DNS Filter的服务发现模式。为此，我们单独开发了DNS-F客户端，DNS-F客户端跟应用部署在同一个主机或容器内。其工作原理如下图所示：</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/7601/1525185700084-16f2c073-0c2b-49c0-b85b-ce2ca1163326.png\" alt=\"image.png | left | 716x392\"></p>\n<ul>\n<li>首先，应用ServiceA通过DNS查询获取到ServiceB的可用IP列表</li>\n<li>DNS-F会拦截到ServiceA的查询请求，判断自己是否该查询的答案，如果有（服务已在VIPServer中注册）则直接返回IP列表；</li>\n<li>如果查询的服务在VIPServer中没有注册，DNS-F把DNS查询转发给系统的nameserver，由真正的DNS系统解析；</li>\n</ul>\n<h2>总结</h2>\n<p>本文介绍了微服务架构中如何基于DNS做服务发现。首先，介绍了服务法注册与发现的概念、服务注册与发现的几种方式及其优缺点；然后，介绍基于DNS的服务发现的两种方式及其优缺点；最后，介绍了阿里巴巴基于DNS做服务发现的实践，主要是基于自研的软负载系统VIPServer。基于DNS的服务发现要解决的问题远不止本文描述的这些，敬请期待后续系列文章（：。</p>\n"
    },
    {
      "filename": "Nacos-is-Coming.md",
      "__html": "<h1>支持Dubbo生态发展，阿里巴巴启动新的开源项目 Nacos</h1>\n<h2>贡献Dubbo生态，阿里Nacos开源计划</h2>\n<p>在上周六的Aliware技术行上海站Dubbo开发者沙龙上，阿里巴巴高级技术专家郭平(坤宇)宣布了阿里巴巴的一个新开源计划，阿里巴巴计划在7月份开启一个名叫Nacos的新开源项目, 在活动演讲中，坤宇介绍了这个开源项目的初衷，他表示 “将通过Nacos项目将阿里巴巴在建设共享服务体系中使用的服务发现、配置及服务管理平台贡献给开源社区，通过打造Dubbo + Nacos的经典组合进一步释放Dubbo在云原生及Service Mesh时代中，在大规模微服务治理、流量治理、服务集成与服务共享等服务平台能力建设上的威力，同时Nacos会非常关注对主流开源社区，如Spring Cloud和Kubernetes云原生体系的无缝对接与支持”。该项目预计在7月中旬之前开放首个测试预览版本，并计划在未来6~8个月release的0.8版本开始达到生产可用的状态。</p>\n<p>活动的 <a href=\"http://www.itdks.com/dakashuo/playback/2307\">视频回放</a></p>\n<h2>什么是 Nacos /nɑ:kəʊs/?</h2>\n<p>Nacos 是阿里巴巴的新开源项目，其核心定位是 “一个更易于帮助构建云原生应用的动态服务发现、配置和服务管理平台”。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530083408486-b4698533-8e69-4e0a-9f3a-fdb1cdfbd290.png\" alt=\"Screen Shot 2018-06-27 at 15.09.35.png\"></p>\n<p>Nacos 有三大主要功能:</p>\n<ul>\n<li>服务发现与服务管理</li>\n</ul>\n<p>在采用以“服务(Service)”为中心的诸如微服务及云原生方式的现代应用架构时，动态服务发现至关重要。 Nacos同时支持基于DNS和基于RPC（如Dubbo/gRPC）的服务发现，并为您提供服务的实时的健康检查以防止将请求发送给不健康的主机，基于Nacos您也可以更方便的实现服务断路器。Nacos提供的强大的服务的元数据管理，路由及流量管理策略也能够帮助您更好的构建更强壮的微服务平台。</p>\n<ul>\n<li>动态配置管理</li>\n</ul>\n<p>动态配置服务允许您在所有环境中以集中和动态的方式管理所有应用程序或服务的配置。动态配置消除了配置更新时重新部署应用程序和服务的需要。可以更方便的帮助您实现无状态服务，更轻松地实现按需弹性扩展服务实例。</p>\n<ul>\n<li>动态DNS服务</li>\n</ul>\n<p>支持权重路由的动态DNS服务使您可以更轻松地在数据中心内的生产环境中实施中间层负载平衡，灵活的路由策略，流量控制和简单的DNS解析服务，帮助您更容易的实现DNS-based服务发现。</p>\n<h2>为什么开源 Nacos</h2>\n<p>阿里巴巴为什么选择这么一个时间点开源Nacos，其背后的思考是什么，坤宇也给出了详细的解读，在演讲中，坤宇表示主意基于以下几点:</p>\n<ul>\n<li><strong>围绕着Service为中心的分布式基础设施正在变的越来越重要</strong></li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077340492-513a25c0-51d0-494f-b39d-6f615d7e3915.png\" alt=\"Screen Shot 2018-06-27 at 13.28.09.png\"></p>\n<p>世界正在变的更快，创新和市场竞争的节奏正在变得愈发剧烈，如何超快速实现业务增长成为商业竞争的主旋律，几乎一夜之间共享单车就火遍全国，不到几年滴滴就改变了我们的打车方式，腾讯三班倒实现全民“吃鸡”，现在企业估值在从0到100亿所需的时间越来越短，而企业的平均寿命从标普的数据来看却从上世纪60年代的60年下降到了今天的15年，一切都表示创新和竞争的速度和烈度在加强。</p>\n<p>另一方面技术基础设施的敏捷和有效性在商业成功的要素上占据的比重越来越大，云计算在资源和服务交付模式上的变革，带来了效率的革命性提升，带来了更敏捷的基础设施（创业不用再买机器并找机房托管，从以前的半年准备周期到现在在云上的小时级创建全套服务），而在应用架构层面，微服务架构模式带来的灵活性、韧劲，快速组合和聚合原子服务从而创新，给业务快速创新和试错提供了条件，已经被越来越多的应用平台证明其有效性，技术基础设施的更敏捷，给商业的敏捷和商业的竞争优势提供了基础。</p>\n<p>在今天，无论是云计算，微服务还是围绕Kubernetes为中心的云原生，都在强调以“服务”为内核的应用架构模式，如果说15年前我们在讨论“一切皆是对象”构建单体系统，那么今天我们就是在谈论“一切皆是服务”，10年前淘宝服务化改造顺应了这种趋势，8年前微服务架构思想也顺应了这个趋势，今天面向“服务”的各种分布式基础设施正在变得越来越重要，站在阿里巴巴10年的服务化发展经验上看，在大规模服务发现和服务治理和服务共享领域现有的开源解决方案是不是都已经非常完美了呢？根据阿里巴巴服务化走过的这些年的生产经验来看，我们觉得并没有。</p>\n<ul>\n<li><strong>阿里巴巴在 &quot;共享服务体系&quot; 建设上的经验可以在各个行业大规模复用</strong></li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077414293-3d22cd01-6e3c-49d1-82ea-ad2f8d443cf9.png\" alt=\"Screen Shot 2018-06-27 at 13.29.52.png\"></p>\n<p>阿里巴巴中台理念和体系，与云原生在精神的“道”上完全契合，即“厚技术平台，薄应用” 支持业务的快速创新与试错，从而赢得市场，中台体系倡导双引擎架构，略过“大数据”不谈，但看业务中台，就是一个大的以“服务”为中心的共享服务平台，在线服务沉淀业务数据，同步到大数据平台计算和挖掘，大数据平台则通过数据回馈，指导业务及服务的创新，支成可沉淀和可共享“服务”体系的服务注册与服务治理平台是这个体系的关键要素之一。</p>\n<ul>\n<li><strong>“服务治理，服务沉淀、服务共享和服务的可持续发展”是“共享服务体系”的核心价值主张</strong></li>\n</ul>\n<p>支持创新从小苗长成参天大树，服务平台不断演进，这一切需要一个强大的服务平台和服务基础设施的支撑。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077444757-d36f344d-d3ae-4df0-808f-8500b4320e8d.png\" alt=\"Screen Shot 2018-06-27 at 13.30.25.png\"></p>\n<ul>\n<li><strong>阿里巴巴将通过Dubbo + Nacos以及一系列开源项目打造服务发现、服务及流量管理、服务共享平台</strong></li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077476468-4ce30dd9-0da7-414a-875d-acc151739c1d.png\" alt=\"Screen Shot 2018-06-27 at 13.30.58.png\"></p>\n<h2>Nacos 与 主流开源生态的关系</h2>\n<p>Nacos 不会是个封闭的体系，除了对于阿里开源生态体系如Dubbo等自身的支持，也非常强调融入其它的开源生态，这里就包括Java的微服务生态体系Spring Cloud，Kubernetes/CNCF云原生生态体系，正如Nacos的未来全景图展示的那样</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077509217-d4fec359-8a41-4c51-a986-c44e06d16950.png\" alt=\"Screen Shot 2018-06-27 at 13.31.30.png\"></p>\n<ul>\n<li>Dubbo + Nacos， 专为Dubbo而生的注册中心与配置中心</li>\n</ul>\n<p>在阿里巴巴生产环境上，Dubbo和Nacos天然就是长在一起的，因为Nacos的缺失，传统的注册中心解决方案让Dubbo在服务治理、流量治理、服务运营和管理等方面的威力被限制和削弱了，Nacos的开源和开放会在采用Dubbo的用户环境中释放这些威力</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077613925-04d767fd-ec95-4fe2-8249-ce8650fbe372.png\" alt=\"Screen Shot 2018-06-27 at 13.31.57.png\"></p>\n<ul>\n<li>Nacos 会完全兼容Spring Cloud</li>\n</ul>\n<p>Nacos会无缝支持Spring Cloud，为Spring Cloud用户其提供更简便的配置中心和注册中心的解决方案，使用Nacos不用再仅仅为服务和配置就需要在生产上hold住 Eureka，Spring Cloud Config Server，Git，RabbitMQ/Kafka 起码四个开源产品。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077646935-7ca8a74d-59e5-4e01-865e-06fd5631e623.png\" alt=\"Screen Shot 2018-06-27 at 13.33.40.png\"></p>\n<ul>\n<li>Nacos 支持Kubernetes DNS-based Service Discovery</li>\n</ul>\n<p>在演讲中坤宇也表示，阿里巴巴这么多年在VIPServer DNS-based Service Discovery上的实践证明，在云原生时代，应用会更关注与基础设施的解耦合、多语言乃至多云的诉求，服务发现的未来一定是基于标准的DNS协议做，而不是像Eureka或者像ZooKeeper这样的私有API或者协议做, 同时在云上，在服务发现场景中，注册中心更关注的是可用性而不是数据一致性，所以Nacos会首推DNS-based Servcie Discovery，并优先关注可用性，而这也正是Nacos可以无缝融合进Kubernetes服务发现体系的原因所在</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077691493-4dae9605-6886-4307-bc9d-16ba1cceb7ac.png\" alt=\"Screen Shot 2018-06-27 at 13.34.18.png\"></p>\n<ul>\n<li>Nacos 会填补Spring Cloud 体系与 Kubernetes 体系的鸿沟</li>\n</ul>\n<p>未来会有越来越多java生态的用户会选择 Kubernetes+Spring Cloud 组合，但不幸的是，在服务发现和配置管理的解决方案上，这2个体系都采用了完全不同的方案，这给同时采用2个体系的用户在注册中心和配置中心的需求上带来了非常大的不必要的复杂性。Nacos会尝试填补2者的鸿沟，以便在2套体系下可以采用同一套服务发现和配置管理的解决方案，这将大大的简化使用和维护的成本。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077731231-85a58c0e-ff5c-45a1-9bc8-b989e2b09d55.png\" alt=\"Screen Shot 2018-06-27 at 13.35.05.png\"></p>\n<ul>\n<li>Nacos 与 Service Mesh</li>\n</ul>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077754298-39bbbdd0-c7af-434a-b189-8fd7ac958b43.png\" alt=\"Screen Shot 2018-06-27 at 13.35.38.png\"></p>\n<h2>Nacos 部分特性预览</h2>\n<p>同时在会上，坤宇对Nacos 1.0版本的部分特性给大家做了预览</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077784470-27f0e268-8d1f-443a-b1a5-3d59e15ab8f4.png\" alt=\"Screen Shot 2018-06-27 at 13.36.06.png\"></p>\n<p>每个产品都有自己的风格和标签，坤宇在演讲中表示，团队会通过持续的贡献和努力，希望未来给Nacos贴上四个方面的标签</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077851626-25444089-c027-4b89-a606-2e693b4e052d.png\" alt=\"Screen Shot 2018-06-27 at 13.37.10.png\"></p>\n<p>在部署形态上，Nacos会支持多种部署形态，包括注册中心与配置中心的分离部署，同时在阿里云上提供Nacos的SaaS化部署服务，感兴趣的可以直接在阿里云上开通账户免费体验Nacos服务，在开源与商业化版本差别上，商业化的ACM以及EDAS ANS更强调与阿里云其它云服务以及其它Aliware PaaS的商业产品的集成体验以及提供商业服务。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077875162-26cfa0e2-3da0-473e-98b9-190aa6d6d97a.png\" alt=\"Screen Shot 2018-06-27 at 13.36.37.png\"></p>\n<h2>Nacos 的主要产品里程碑及计划</h2>\n<blockquote>\n<p>罗马不是一天建成的，吾将上下而求索</p>\n</blockquote>\n<p>因为Nacos是脱胎于阿里巴巴的生产代码，整体体系非常庞杂，在代码梳理、重构和剥离与内部的耦合上是一个渐进的过程，所以@坤宇特别提醒社区在Nacos 0.8达到生产可用状态前，不建议用于生产，不过可以在开发和测试环境尝试使用，在0.8版本开始，大家可以放心的用于生产环境，Nacos整体研发计划是在未来6-8个月将达到生产可用的状态，未来会很快启动将Nacos贡献给开源基金会进一步社区化发展。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077912860-52c7fea4-5285-45a0-a3b8-73ba010c53da.png\" alt=\"Screen Shot 2018-06-27 at 13.38.13.png\"></p>\n<h2>Nacos 与相关开源产品的对比</h2>\n<blockquote>\n<p>君子和而不同</p>\n</blockquote>\n<p>如上面对整体业务及技术形式的判断，我们可以看到 Nacos 与同类竞品的主要不同主要在于理念，@坤宇介绍说，严格来说这些市面上的产品并不与Nacos完全对标，只是与Nacos里面的服务发现部分对标，Nacos的未来更看重在这些产品的基础上构建服务平台的能力，而不仅仅是在基础的服务发现能力上。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077938157-00e6b34f-9a16-498c-9b89-cc48f6d6bb73.png\" alt=\"Screen Shot 2018-06-27 at 13.38.41.png\"></p>\n<h2>社区化发展，欢迎加入并贡献社区</h2>\n<blockquote>\n<p>DISS is cheap, show me your hand\n比吐槽更重要的是搭把手，参与社区一起发展Nacos</p>\n</blockquote>\n<p>与阿里巴巴早期的开源不同，阿里巴巴新一轮的开源包括RocketMQ，Pouch Container，Dubbo, Nacos等开源产品更强调社区化的发展与社区的多样性，鼓励更多的公司和更多的开发者参与到开源项目中来，依托于社区将产品做得更好，同时一开始就会关注国际化，与国外同类产品的直面竞争。</p>\n<p>Nacos初步计划，在第一年就吸收至少超过5名来自其它公司的PMC,至少10名的外部Committer, 而且Nacos处在项目开源的初期，有大把的空间让有想法、有热情、有能力的开发者参与进来，Nacos本身在很多方面都急需要社区的帮助，这里面就急需包括前端及UI建设，Spring Cloud、Kubernetes、Service Mesh体系融合与集成，多语言客户端等各方面的技术领导者的参与，如果您对Nacos这个开源项目感兴趣，欢迎加入Nacos社区。你可以通过扫“超哥”的微信二维码，让“超哥” 帮你加入 “Nacos社区交流群”</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530077965587-8f4e3100-bdd4-469a-9ea0-7af7061bc9ef.png\" alt=\"Screen Shot 2018-06-27 at 13.39.09.png\"></p>\n<h2>感谢所有未来给我们帮助的朋友</h2>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/15914/1530078016801-65ce72c1-c135-42b6-8076-a046f7829ba7.png\" alt=\"Screen Shot 2018-06-27 at 13.39.40.png\"></p>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://yq.aliyun.com/articles/601745\">阿里巴巴为什么不用 ZooKeeper 做服务发现？</a></li>\n<li><a href=\"https://yq.aliyun.com/articles/226238?spm=5176.163362.847322.3.5a8925397in790\">现代应用架构中的配置管理面临的挑战和应对之道</a></li>\n<li><a href=\"http://jm.taobao.org/2016/06/02/zhangwensong-and-load-balance/\">章文嵩博士和他背后的负载均衡帝国</a></li>\n<li><a href=\"https://blog.csdn.net/heyc861221/article/details/80126013\">VIPServer：阿里智能地址映射及环境管理系统详解</a></li>\n</ul>\n"
    },
    {
      "filename": "alibaba-configserver.md",
      "__html": "<h1>《阿里巴巴服务注册中心产品ConfigServer 10年技术发展回顾》</h1>\n<p>作者：阿里巴巴高级技术专家，许真恩(慕义)</p>\n<p>文章概要：本文简单描述了Eureka1.0存在的架构问题，Eureka2.0设想的架构。详细回顾了阿里巴巴的服务注册中心ConfigServer产品从2008年建设元年至今经历的关键架构演进。通过这个文章你会对基于AP模式的注册中心在技术发展过程中将会碰到的问题有所感知。</p>\n<h1>Eureka1.0架构存在的问题</h1>\n<p>Eureka作为Netflix公司力推和SpringCloud微服务标配的注册中心开源解决方案，其<span data-type=\"color\" style=\"color:rgb(36, 41, 46)\">Eureka 2.0 (Discontinued)的消息在社区引起了不小的骚动；其实早在2015年社区就已经放出了2.0架构的升级设想，但是3年的时间过去，等到的确是Discontinued的消息，虽然2.0的代码在github的主页上也已经放出，但是告诫用户要自行承担当中的使用风险。我想不会有人真的把2.0直接投入到生产中使用。</span></p>\n<p>对于为什么要做Eureka2.0，其官方的wiki中的Why Eureka 2.0和Eureka 2.0 Improvements做了如下的说明</p>\n<ul>\n<li>Only support homogenous client views: Eureka servers only expects the client to always get the entire registry and does not allow to get specific applications/VIP addresses. This imposes a memory limitation on all clients registering with Eureka, even if they need a very small subset of the Eureka’s registry.</li>\n<li>Only supports scheduled updates: Eureka client follows a poll model to fetch updates from the server routinely. This imposes an overhead on the client of doing a scheduled call to the server, even if there are no changes and also delays the updates by the poll interval.</li>\n<li>Replication algorithm limits scalability: Eureka follows a broadcast replication model i.e. all the servers replicate data and heartbeats to all the peers. This is simple and effective for the data set that eureka contains however replication is implemented by relaying all the HTTP calls that a server receives as is to all the peers. This limits scalability as every node has to withstand the entire write load on eureka.</li>\n</ul>\n<p>Although Eureka 2.0 provides a more richer feature set, the above limitations are the major driving factors for the changes proposed in this version.\nBased on the above motivations, Eureka 2.0 achieves the following improvements:</p>\n<ul>\n<li>Interest based subscription model for registry data: A client of Eureka is able to select a part of the instance registry in which it is interested in and the eureka server only sends information about that part of the registry. Eg: A client can say I am only interested in application “WebFarm” and then the server will only send information about WebFarm instances. Eureka server provides various selection criterions and a way to update this interest set dynamically.</li>\n<li>Push model from the server for any changes to the interest set: Instead of the current pull model from the client, Eureka servers pushes updates for changes to the interest set, to the client.</li>\n<li>Optimized replication: As Eureka 1.0, Eureka 2.0 also follows a broadcast replication model i.e. every node replicates data to all other nodes. However, the replication algorithm is much more optimized eliminating the need of sending heartbeats for every instance in the registry. This drastically reduce the replication traffic and achieves much higher scalability.</li>\n<li>Auto-scaled Eureka servers: Eureka 2.0 divides the read (discovery of data) and write (registration) concerns into separate clusters. Since, the write load is predictable (proportional to the number of instances in a region), the write cluster is pre-scaled. On the other hand, read load is unpredictable (proportional to subscriptions from clients) and hence the read farm is auto-scaled.</li>\n<li>Audit log: Eureka 2.0 provides an elaborate audit log for any change done to the registry. This helps Eureka owners as well as users to get insight into debugging the state of individual application instances as exists in Eureka. The audit log by default is persisted to a log file, but different persistent storages can be plugged-in.</li>\n<li>Dashboard: Eureka 2.0 provides a rich dashboard (as opposed to very rudimentary dashboard of Eureka 1.0) with insights into Eureka internals with respect to registry views, server health, subscription state, audit log, etc.</li>\n</ul>\n<p>简单翻译总结，也就是Eureka1.0的架构主要存在如下的不足：</p>\n<ul>\n<li>订阅端拿到的是服务的全量的地址：这个对于客户端的内存是一个比较大的消耗，特别在多数据中心部署的情况下，某个数据中心的订阅端往往只需要同数据中心的服务提供端即可。</li>\n<li>pull模式：客户端采用周期性向服务端主动pull服务数据的模式（也就是客户端轮训的方式），这个方式存在实时性不足以及无谓的拉取性能消耗的问题。</li>\n<li>一致性协议：Eureka集群的多副本的一致性协议采用类似“异步多写”的AP协议，每一个server都会把本地接收到的写请求（register/heartbeat/unregister/update）发送给组成集群的其他所有的机器（Eureka称之为peer），特别是hearbeat报文是周期性持续不断的在client-&gt;server-&gt;all peers之间传送；这样的一致性算法，导致了如下问题\n<ul>\n<li>每一台Server都需要存储全量的服务数据，Server的内存明显会成为瓶颈。</li>\n<li>当订阅者却来越多的时候，需要扩容Eureka集群来提高读的能力，但是扩容的同时会导致每台server需要承担更多的写请求，扩容的效果不明显。</li>\n<li>组成Eureka集群的所有server都需要采用相同的物理配置，并且只能通过不断的提高配置来容纳更多的服务数据</li>\n</ul>\n</li>\n</ul>\n<p>Eureka2.0主要就是为了解决上述问题而提出的，主要包含了如下的改进和增强</p>\n<ul>\n<li>数据推送从pull走向push模式，并且实现更小粒度的服务地址按需订阅的功能。</li>\n<li>读写分离：写集群相对稳定，无需经常扩容；读集群可以按需扩容以提高数据推送能力。</li>\n<li>新增审计日志的功能和功能更丰富的Dashboard。</li>\n</ul>\n<p>Eureka1.0版本存在的问题以及Eureka2.0架构设想和阿里巴巴内部的同类产品ConfigServer所经历的阶段非常相似（甚至Eureka2.0如果真的落地后存在的问题，阿里巴巴早已经发现并且已经解决），下面我带着你来回顾一下我们所经历过的。</p>\n<h1>阿里巴巴服务注册中心ConfigServer技术发现路线</h1>\n<p>阿里巴巴早在2008就开始了服务化的进程，在那个时候就就已经自研出服务发现解决方案（内部产品名叫ConfigServer）。</p>\n<p>当2012年9月1号Eureka放出第一个1.1.2版本的时候，我们把ConfigServer和Eureka进行了深度的对比，希望能够从Eureka找到一些借鉴来解决当时的ConfigServer发展过程中碰到的问题（后面会提到）；然而事与愿违，我们已经发现Eureka1.x架构存在比较严重的扩展性和实时性的问题（正如上面所描述的），这些问题ConfigServer当时的版本也大同小异的存在，我们在2012年底对ConfigServer的架构进行了升级来解决这些问题。</p>\n<p>当2015年Eureka社区放出2.0架构升级的声音的时候，我们同样第一时间查看了2.0的目标架构设计，我们惊奇的发现Eureka的这个新的架构同2012年底ConfigServer的架构非常相似，当时一方面感慨“英雄所见略同”，另一方也有些失望，因为ConfigServer2012年的架构早就无法满足阿里巴巴内部的发展诉求了。</p>\n<p>下面我从头回顾一下，阿里巴巴的ConfigServer的技术发展过程中的几个里程碑事件。</p>\n<h2>2008年，无ConfigServer的时代</h2>\n<p>借助用硬件负载设备F5提供的vip功能，服务提供方只提供vip和域名信息，调用方通过域名方式调用，所有请求和流量都走F5设备。</p>\n<p>遇到的问题：</p>\n<ul>\n<li>负载不均衡：对于长连接场景，F5不能提供较好的负载均衡能力。如服务提供方发布的场景，最后一批发布的机器，基本上不能被分配到流量。需要在发布完成后，PE手工去断开所有连接，重新触发连接级别的负载均衡。</li>\n<li>流量瓶颈：所有的请求都走F5设备，共享资源，流量很容易会打满网卡，会造成应用之间的相互影响。</li>\n<li>单点故障：F5设备故障之后，所有远程调用会被终止，导致大面积瘫痪。</li>\n</ul>\n<h2>2008年，ConfigServer单机版V1.0</h2>\n<p>单机版定义和实现了服务发现的关键的模型设计（包括服务发布，服务订阅，健康检查，数据变更主动推送，这个模型至今仍然适用），应用通过内嵌SDK的方式接入实现服务的发布和订阅；这个版本很重要的一个设计决策就是服务数据变更到底是pull还是push的模式，我们从最初就确定必须采用push的模式来保证数据变更时的推送实时性（Eureka1.x的架构采用的是pull的模式）</p>\n<p>当时，HSF和Notify就是采用单机版的ConfigServer来完成服务的发现和topic的发现。单机版的ConfigServer和HSF、Notify配合，采用服务发现的技术，让请求通过端到端的方式流动，避免产生流量瓶颈和单点故障。ConfigServer可以动态的将服务地址推送到客户端，服务调用者可以知道所有提供此服务的机器，每个请求都可以通过随机或者轮询的方式选择服务端，做到请求级别的负载均衡。这个版本已经能很好的解决F5设备不能解决的三个问题。</p>\n<p>但是单机版本的问题也非常明显，不具备良好的容灾性；</p>\n<h2>2009年初，ConfigServer单机版V1.5</h2>\n<p>单机版的ConfigServer所面临的问题就是当ConfigServer在发布/升级的时候，如果应用刚好也在发布，这个时候会导致订阅客户端拿不到服务地址的数据，影响服务的调用；所以当时我们在SDK中加入了本地的磁盘缓存的功能，应用在拿到服务端推送的数据的时候，会先写入本地磁盘，然后再更新内存数据；当应用重启的时候，优先从本地磁盘获取服务数据；通过这样的方式解耦了ConfigServer和应用发布的互相依赖；这个方式沿用至今。（我很惊讶，Eureka1.x的版本至今仍然没有实现客户端磁盘缓存的功能，难道Eureka集群可以保持100%的SLA吗？）</p>\n<h2>2009年7月，ConfigServer集群版本V2.0</h2>\n<p>ConfigServer的集群版本跟普通的应用有一些区别：普通的应用通过服务拆分后，已经属于无状态型，本身已经具备良好的可扩展性，单机变集群只是代码多部署几台；ConfigServer是有状态的，内存中的服务信息就是数据状态，如果要支持集群部署，这些数据要不做分片，要不做全量同步；由于分片的方案并没有真正解决数据高可用的问题（分片方案还需要考虑对应的failover方案），同时复杂度较高；所以当时我们选择了单机存储全量服务数据全量的方案。为了简化数据同步的逻辑，服务端使用客户端模式同步：服务端收到客户端请求后，通过客户端的方式将此请求转发给集群中的其他的ConfigServer，做到同步的效果，每一台ConfigServer都有全量的服务数据。</p>\n<p>这个架构同Eureka1.x的架构惊人的相似，所以很明显的Eureka1.x存在的问题我们也存在；当时的缓解的办法是我们的ConfigServer集群全部采用高配物理来部署。</p>\n<h2>2010年底，ConfigServer集群版V2.5</h2>\n<p>基于客户端模式在集群间同步服务数据的方案渐渐无以为继了，特别是每次应用在发布的时候产生了大量的服务发布和数据推送，服务器的网卡经常被打满，同时cmsgc也非常频繁；我们对数据同步的方案进行了升级，去除了基于客户端的同步模式，采用了批量的基于长连接级别的数据同步+周期性的renew的方案来保证数据的一致性（这个同步方案当时还申请了国家专利）；这个版本还对cpu和内存做了重点优化，包括同步任务的合并，推送任务的合并，推送数据的压缩，优化数据结构等；</p>\n<p>这个版本是ConfigServer历史上一个比较稳定的里程碑版本。</p>\n<p><span data-type=\"color\" style=\"color:rgb(36, 41, 46)\">但是随着2009年天猫独创的双十一大促活动横空出世，服务数量剧增，应用发</span>布时候ConfigServer集群又开始了大面积的抖动，还是体现在内存和网卡的吃紧，甚至渐渐到了fullgc的边缘；为了提高数据推送能力，需要对集群进行扩容，但是扩容的同时又会导致每台服务器的写能力下降，我们的架构到了“按下葫芦浮起瓢”的瓶颈。</p>\n<h2>2012年底，ConfigServer集群版V3.0</h2>\n<p>在2011年双十一之前我们完成了V3架构的落地，类似Eureka2.0所设计的读写分离的方案，我们把ConfigServer集群拆分成session和data两个集群，客户端分片的把服务数据注册到session集群中，session集群会把数据异步的写到data集群，data集群完成服务数据的聚合后，把压缩好的服务数据推送到session层缓存下来，客户端可以直接从session层订阅到所需要的服务数据；这个分层架构中，session是分片存储部分的服务数据的（我们设计了failover方案），data存储的是全量服务数据（天生具备failover能力）；</p>\n<p>data集群相对比较稳定，不需要经常扩容；session集群可以根据数据推送的需求很方便的扩容（这个思路和Eureka2.0所描述的思路是一致的）；session的扩容不会给data集群带来压力的增加。session集群我们采用低配的虚拟机即可满足需求，data由于存储是全量的数据我们仍然采用了相对高配的物理机（但是同V2.5相比，对物理机的性能要求已经答复下降）</p>\n<p>这个版本也是ConfigServer历史上一个划时代的稳定的大版本。</p>\n<h2>2014年，ConfigServer集群版V3.5</h2>\n<p>2013年，阿里巴巴开始落地了异地多活方案，从一个IDC渐渐往多个IDC跨越，随之而来的对流量的精细化管控要求越来越高（比如服务的同机房调用，服务流量的调拨以支持灰度能力等），基于这个背景ConfigServer引入了服务元数据的概念，支持对服务和IP进行元数据的打标来满足各种各样的服务分组诉求。</p>\n<h2>2017年，ConfigServer集群版V4.0</h2>\n<p>V3版本可见的一个架构的问题就是data集群是存储全量的服务数据的，这个随着服务数的与日俱增一定会走到升级物理机配置也无法解决问题的情况（我们当时已经在生产使用了G1的垃圾回收算法），所以我们继续对架构进行升级，基于V3的架构进行升级其实并不复杂；session层的设计保持不变，我们把data进行分片，每一个分片同样按照集群的方式部署以支持failover的能力；</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ConfigServer</th>\n<th>Eureka</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2008年</td>\n<td>V1.0：单机版，定义了服务发现的领域模型</td>\n<td></td>\n</tr>\n<tr>\n<td>2009年初</td>\n<td>V1.5：应用和ConfigServer集群发布解耦</td>\n<td></td>\n</tr>\n<tr>\n<td>2009年7月</td>\n<td>V2.0：基于客户端模式同步数据，支持集群部署</td>\n<td></td>\n</tr>\n<tr>\n<td>2010年底</td>\n<td>V2.5：优化集群间数据同步模式，申请国家专利。</td>\n<td></td>\n</tr>\n<tr>\n<td>2012年9月1号</td>\n<td></td>\n<td>Eureka1.0正式开源</td>\n</tr>\n<tr>\n<td>2012年底</td>\n<td>V3.0：支持session和data分层部署</td>\n<td></td>\n</tr>\n<tr>\n<td>2014年</td>\n<td>V3.5：支持异地多活等细分场景</td>\n<td></td>\n</tr>\n<tr>\n<td>2015年</td>\n<td></td>\n<td>Eureka2.0架构升级方案公布</td>\n</tr>\n<tr>\n<td>2017年</td>\n<td>V4.0：支持data分片能力</td>\n<td></td>\n</tr>\n<tr>\n<td>2018年7月</td>\n<td></td>\n<td>Eureka2.0架构升级宣布停止</td>\n</tr>\n</tbody>\n</table>\n<h1>Nacos</h1>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">作为同属于AP类型的注册中心，Eureka和ConfigServer发展过程中所面临的诸多问题有很大的相似性，但是阿里巴巴这些年业务的跨越式发展，每年翻番的服务规模，不断的给ConfigServer的技术架构演进带来更高的要求和挑战，我们有更多的机会在生产环境发现和解决一个个问题的过程中，做架构的一代代升级。我们正在计划通过开源的手段把我们这些年在生产环境上的实践经验通过Nacos(</span></span><a href=\"http://nacos.io\">链接</a>)<span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">产品贡献给社区，一方面能够助力和满足同行们在微服务落地过程当中对工业级注册中心的诉求，另一方面也希望通过开源社区及开源生态的协同发展给ConfigServer带来更多的可能性。</span></span></p>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">炎炎夏日，在Eureka 2.0 (Discontinued) 即将结束的时候，在同样的云原生时代，Nacos却正在迎来新生，技术演进和变迁的趣味莫过于此。</span></span></p>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">Nacos将努力继承Eureka未竟的遗志，扛着AP系注册中心的旗帜继续前行，不同的是Nacos更关注DNS-based Service Discovery以及与Kubernetes体系的融会贯通。</span></span></p>\n<p><span data-type=\"color\" style=\"color:rgb(25, 31, 37)\"><span data-type=\"background\" style=\"background-color:rgb(255, 255, 255)\">我们看不透未来，却仍将与同行们一起上下求索，砥砺前行。</span></span></p>\n<p>最后附上Nacos的架构图。</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/12425/1530856061648-7180b97f-b61d-4127-924e-a0743b9e9d5a.png\" alt=\"屏幕快照 2018-06-28 下午4.58.59.png | center | 748x544\"></p>\n"
    },
    {
      "filename": "nacos.md",
      "__html": "<h1>Nacos 0.1.0 �汾Review ����</h1>\n<h2>I. ������б�</h2>\n<ul>\n<li>�Ķ������İ����ҹ���bug, ���С�Ӣ�Ĺ����Ὠ���Խ���</li>\n<li>�Ķ���Ӣ�ĵ������ĵ�bug, ���С�Ӣ�ĵ������Ὠ�飨�����ǹ�עӢ�ķ��벻�õĵط�����ΪӢ�Ķ������ǳ���Ա�Լ�ߣ�ģ�</li>\n<li>�����´��� -&gt;������ -&gt; ��Nacos server -&gt; ֹͣNacos server���̣�����Ľ����</li>\n<li>���������Լ����ڵ� Nacos ��Ⱥģʽ������Ľ����</li>\n<li>����ʹ��Nacos Java SDK, ��Java SDK��Ľ�����</li>\n<li>����ʹ��Nacos Open API����OpenAPI��Ľ�����</li>\n<li>���Ը��ݡ���ι���Naocs�ĵ� TODO����һ�� �������̣��������������Ὠ��</li>\n<li>��Nacos�����󡢷�չ�ƻ����뷨��Ҫ���</li>\n</ul>\n<h2>II. ����뷽ʽ</h2>\n<ul>\n<li>\n<p>ɨ�� �����硱 ΢��2΢�룬�ó���������� ��Nacos��������Ⱥ��</p>\n<p><img src=\"https://cdn.yuque.com/lark/0/2018/png/11189/1532004866850-5e03b901-6d76-4380-b7bf-66e227808bdc.png\" alt=\"΢�Ŷ�ά��\"></p>\n</li>\n<li>\n<p>ѡ��I �е�һ�����߶����������</p>\n</li>\n<li>\n<p>�����������BUG֮�󣬰���III�еġ�����Report��ʽ������һ����Ӧ�� github issue, ��ָ�ɸ� @ github�˺�<a href=\"https://github.com/xuechaos\">xuechaos</a></p>\n</li>\n</ul>\n<h2>III. ����Report��ʽ</h2>\n<ul>\n<li>����˵�� TODO</li>\n</ul>\n<h2>IV. ������ʽ</h2>\n<ul>\n<li>��������Ϊ���벢����ͻ�����׵�С��鶨��һЩС��Ʒ���ῼ�ǿ�ݸ���������ͻ�����׵�С���</li>\n<li>�������ᣬ��ϣ���ܱ������ǵİ����ĸм�֮�����һ</li>\n</ul>\n<h2>V. ����˵��</h2>\n<ul>\n<li>���ǲ�ȷ��ÿ��������󶼻ᱻ���ã��������Ǿ����������ͨ���ǻ��ں��ֿ��ǣ����Ľ����������û�в���</li>\n<li>����ͨ���ʼ��б����report issue�ķ�ʽ����������΢��Ⱥ��report���⣬�Ա㽫���ǵĹ�ͨ�����ĵ����͸����׳���</li>\n</ul>\n"
    }
  ]
}